<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://womeimingzi11.github.io</id>
    <title>洗衣机用户不会用洗衣机</title>
    <updated>2020-10-10T04:15:56.339Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://womeimingzi11.github.io"/>
    <link rel="self" href="https://womeimingzi11.github.io/atom.xml"/>
    <subtitle>Why be a researcher if you can be an Engineer?</subtitle>
    <logo>https://womeimingzi11.github.io/images/avatar.png</logo>
    <icon>https://womeimingzi11.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 洗衣机用户不会用洗衣机</rights>
    <entry>
        <title type="html"><![CDATA[amapGeocode: 使用R进行高德地图地理编码/逆编码]]></title>
        <id>https://womeimingzi11.github.io/post/dG9esk683/</id>
        <link href="https://womeimingzi11.github.io/post/dG9esk683/">
        </link>
        <updated>2020-10-08T16:13:37.000Z</updated>
        <summary type="html"><![CDATA[<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
<p>在最近的学习中，大量使用到了地理编码与地理逆编码，然而不幸的是目前广泛在 R 中介绍的能完成该任务的包因为 API 的 Breaking Upgrade 很多功能已经失效了。尤其是意外的发现，目前的包主要由百度地图 API 作数据源，于是为了满足在 R 中使用高德地图 API 进行地理编码与逆编码的需求，开发了 <a href="https://github.com/womeimingzi11/amapGeocode"><code>amapGeocode</code></a> 这款包，如果能对有同样需求的同学有任何帮助，将不胜荣幸。</p>
]]></summary>
        <content type="html"><![CDATA[<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
<p>在最近的学习中，大量使用到了地理编码与地理逆编码，然而不幸的是目前广泛在 R 中介绍的能完成该任务的包因为 API 的 Breaking Upgrade 很多功能已经失效了。尤其是意外的发现，目前的包主要由百度地图 API 作数据源，于是为了满足在 R 中使用高德地图 API 进行地理编码与逆编码的需求，开发了 <a href="https://github.com/womeimingzi11/amapGeocode"><code>amapGeocode</code></a> 这款包，如果能对有同样需求的同学有任何帮助，将不胜荣幸。</p>
<!-- more -->
<h2 id="什么是地理编码逆编码">什么是地理编码/逆编码</h2>
<p>根据<a href="https://www.zhihu.com/people/di-li-xiao-zi">地理小子</a>在文章<a href="https://zhuanlan.zhihu.com/p/22095359">《地理编码与逆地理编码（上）——理解地理编码》</a>中的陈述：</p>
<blockquote>
<p>地理编码指将地名的详细地址以地理坐标（如经纬度)表示的过程。其中，将地址信息映射为地理坐标的过程称之为地理编码；将地理坐标转换为地址信息的过程称之为逆地理编码。如图1.1所示为地理编码和逆地理编码的关系。</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://pic3.zhimg.com/80/d2e918ce2b6e6f44d72e2d7659d60566_1440w.jpg" alt="图1.1-地理小子-《地理编码与逆地理编码（上）——理解地理编码》" loading="lazy"></figure>
<p>无论是在科学研究还是数据处理的过程中，凡是涉及到位置信息的情况，经常会使用到地理编码与逆编码：例如给定样本的地址，将其转换为经纬度以便进行量化计算并进行可视化；亦或者给定样本地址信息，将其按照行政区域进行分类以便进行分组并统计。</p>
<h2 id="如何在-r-中进行地理编码逆编码">如何在 R 中进行地理编码/逆编码</h2>
<p>目前进行地理编码/逆编码的方法通常是使用地图供应商提供的 API 获取信息。在 R 中目前常使用 <a href="https://CRAN.R-project.org/package=tidygeocoder">tidygeocoder</a>，<a href="https://github.com/badbye/baidumap">baidumap</a> 以及 <a href="https://github.com/ChrisMuir/baidugeo">baidugeo</a> 等 Package 完成这类任务。然而，其中 <a href="https://CRAN.R-project.org/package=tidygeocoder">tidygeocoder</a> 使用国际地理信息供应商，其准确度以及信息覆盖程度尚不能令人满意；而后两者因为百度地图 API 的升级，导致目前 Package 不能被新用户调用（其中前者笔者已经提交了 <a href="https://github.com/badbye/baidumap/pull/20">Request</a>，不过是否会被合并还需要进一步等待；后者<a href="https://github.com/ChrisMuir/baidugeo/pull/4">笔者正在和开发者进行更新</a>，具体修复时间还需等待）。</p>
<p>此外，通过笔者在实际项目的使用过程中发现，在相当多的情况下，高德地图提供的地理编码/逆编码信息似乎好于百度地图，当然因为没有进行客观的对照分析，仅作为个人体验以作参考<s>，如有谬误，您说的对</s>。</p>
<h2 id="amapgeocode">amapGeocode</h2>
<p>基于以上原因，笔者使用<a href="https://lbs.amap.com/api/webservice/guide/api/georegeo">高德开放平台</a> API 编写了用于 R 的地理编码/逆编码的 Package <a href="https://cran.r-project.org/package=amapGeocode">amapGeocode</a>。代码托管于 <a href="https://github.com/womeimingzi11/amapGeocode">Github</a>。具体的高德文档可以查看<a href="https://lbs.amap.com/">这里</a>和<a href="https://lbs.amap.com/api/webservice/summary/">这里</a>。</p>
<p>目前 amapGeocode 支持通过 <code>getCoord()</code> 进行地理编码，即根据地址获取坐标等信息；以及通过 <code>getLocation()</code> 进行地理逆编码，即根据经纬度获取地址等信息。此外还可以通过 <code>getAdmin()</code> 获取给定行政区其下属行政区（<code>JSON</code> 和 <code>XML</code> 支持多级行政区）。</p>
<p>上述 function 支持输入单条的文本地址/坐标/行政区，同时也支持批量输入信息，其默认返回结果为 <a href="https://tibble.tidyverse.org/">tibble</a>， 同时也可以返回原生的 <code>JSON</code> 格式以及 <code>XML</code> 格式，此时批量查询的地址将以 <code>list</code> 的结构保存。</p>
<h3 id="安装">安装</h3>
<p>目前可以通过下列命令从 CRAN 安装稳定版：</p>
<pre><code class="language-r">install.packages(&quot;amapGeocode&quot;)
</code></pre>
<p>此外也可以通过 Github 安装开发版：</p>
<pre><code class="language-r">remotes::install_github('womeimingzi11/amapGeocode')
</code></pre>
<h3 id="申请-api-key">申请 API Key</h3>
<p>由于 amapGeocode 使用了高的开放平台的的 API 服务，因此在使用 amapGeocode 之前，用户<strong>必须首先申请</strong> API Key，申请地址如下 <a href="https://console.amap.com/dev/index" class="uri">https://console.amap.com/dev/index</a></p>
<figure data-type="image" tabindex="2"><img src="http://hansblog.test.upcdn.net/Snipaste_2020-10-09_23-14-44.png" alt="" loading="lazy"></figure>
<p>点击<code>管理Key</code>进入管理页面，选择<code>创建新应用</code>，应用名称和用途可以根据自己的实际用途填写。之后在刚刚创建的应用右侧点击添加按钮添加一个新的 API Key，Key 名称可以自行命名，但是注意一定要把服务平台选为 <code>Web服务</code>。之后就能查看申请到的 Key 了。<strong>请注意，Key 的作用相当于访问高德服务的密码，切勿分享给其他人使用。如果怀疑 Key 泄露，可以通过<code>删除</code>按钮撤销该 Key，但同时也意味着您之前在程序中引用的该 Key 也需要一并更换，否者 amapGeocode 无法正常工作。</strong></p>
<figure data-type="image" tabindex="3"><img src="http://hansblog.test.upcdn.net/Snipaste_2020-10-09_23-32-41.png" alt="" loading="lazy"></figure>
<p>此外，截止到 2020 年 10 月 9 日，根据<a href="https://console.amap.com/dev/flow/manage">高德开放平台配额管理页面显示</a>：高德开放平台<strong>地理编码/逆地理编码/行政区查询服务</strong>对于实名认证免费用户分别提供了<strong>单日</strong> 300,000次/300,000次/30,000 次的使用额度，超过此额度后，当日无法继续使用查询服务。理论上来讲，除非主动购买额外的查询次数，否则不会产生费用，但 amapGeocode 开发者暨笔者不对高德开放平台其规定与收费政策作任何保证，请用户自行判断。</p>
<h2 id="使用方法">使用方法</h2>
<h3 id="地理编码">地理编码</h3>
<pre><code class="language-r">library(amapGeocode)
</code></pre>
<p>加载 amapGeocode 后，用户可以选择每次执行命令之时手动指定 <code>key</code> 参数来设定之前申请到的 Key，同时也可以通过下列命令将 key 设置为全局可用，则单条命令无需再次手动输入之前申请到的 Key</p>
<pre><code>options(amap_key = 'REPLACE THIS BY YOUR KEY')
</code></pre>
<p>通过 <code>getCoord()</code> 来获取给定地址的经纬度：</p>
<pre><code class="language-r">getCoord(c(&quot;四川省中医院&quot;, &quot;四川省人民医院&quot;, &quot;兰州大学盘旋路校区&quot;, &quot;成都中医药大学十二桥校区&quot;))
</code></pre>
<pre><code>## # A tibble: 4 x 12
##     lng   lat formatted_addre~ country province city  district township street
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;lgl&gt;    &lt;lgl&gt; 
## 1  104.  30.7 四川省成都市金牛区四川省中医院~ 中国    四川省   成都市~ 金牛区   NA       NA    
## 2  104.  30.7 四川省成都市青羊区四川省人民医~ 中国    四川省   成都市~ 青羊区   NA       NA    
## 3  104.  36.0 甘肃省兰州市城关区兰州大学盘旋~ 中国    甘肃省   兰州市~ 城关区   NA       NA    
## 4  104.  30.7 四川省成都市金牛区成都中医药大~ 中国    四川省   成都市~ 金牛区   NA       NA    
## # ... with 3 more variables: number &lt;lgl&gt;, citycode &lt;chr&gt;, adcode &lt;chr&gt;
</code></pre>
<p>在默认状态下，该命令将返回按输入顺序排序的 tibble 表格。不过用户依然可以通过指定 <code>output = 'JSON'</code> 或 <code>output = 'XML'</code> 并配合 <code>to_table = FALSE</code> 来直接获得高德地图 API 返回的 <code>JSON/XML</code> 格式的结果。</p>
<pre><code class="language-r"># An individual request
res &lt;- getCoord(&quot;成都中医药大学&quot;, output = &quot;XML&quot;, to_table = FALSE)
res
</code></pre>
<pre><code>## {xml_document}
## &lt;response&gt;
## [1] &lt;status&gt;1&lt;/status&gt;
## [2] &lt;info&gt;OK&lt;/info&gt;
## [3] &lt;infocode&gt;10000&lt;/infocode&gt;
## [4] &lt;count&gt;1&lt;/count&gt;
## [5] &lt;geocodes type=&quot;list&quot;&gt;\n  &lt;geocode&gt;\n    &lt;formatted_address&gt;四川省成都市金牛区成都中医 ...
</code></pre>
<p>这类结果可以使用 amapGeocode 内置的 <code>extractCoord()</code> 处理为与之前格式相同的 <code>tibble</code> 格式。</p>
<pre><code class="language-r">extractCoord(res)
</code></pre>
<pre><code>## # A tibble: 1 x 12
##     lng   lat formatted_addre~ country province city  district township street
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;lgl&gt;    &lt;lgl&gt; 
## 1  104.  30.7 四川省成都市金牛区成都中医药大~ 中国    四川省   成都市~ 金牛区   NA       NA    
## # ... with 3 more variables: number &lt;lgl&gt;, citycode &lt;chr&gt;, adcode &lt;chr&gt;
</code></pre>
<h3 id="地理逆编码">地理逆编码</h3>
<p>与地理编码的使用方法基本相同，<code>getLocation()</code> 示例如下：</p>
<pre><code class="language-r">coord &lt;- tibble::tribble(~lat, ~lon, 104.043284, 30.666864, 104.039, 30.66362)
getLocation(coord$lat, coord$lon)
</code></pre>
<pre><code>## # A tibble: 2 x 8
##   formatted_address   country province city  district township citycode towncode
##   &lt;chr&gt;               &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   
## 1 四川省成都市金牛区西安路街道成都中医~ 中国    四川省   成都市~ 金牛区   西安路街道~ 028      5101060~
## 2 四川省成都市青羊区草堂街道四川省医学~ 中国    四川省   成都市~ 青羊区   草堂街道 028      5101050~
</code></pre>
<h3 id="行政区获取">行政区获取</h3>
<p>与地理编码/逆编码方法基本相同，<code>getAdmin()</code>示例如下：</p>
<pre><code class="language-r">getAdmin(c(&quot;四川省&quot;, &quot;兰州市&quot;, &quot;济宁市&quot;))
</code></pre>
<pre><code>## [[1]]
## # A tibble: 21 x 6
##      lng   lat name   level citycode adcode
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; 
##  1  107.  31.9 巴中市 city  0827     511900
##  2  104.  30.7 成都市 city  028      510100
##  3  104.  31.1 德阳市 city  0838     510600
##  4  106.  32.4 广元市 city  0839     510800
##  5  106.  30.5 遂宁市 city  0825     510900
##  6  105.  30.1 资阳市 city  0832     512000
##  7  105.  31.5 绵阳市 city  0816     510700
##  8  107.  30.5 广安市 city  0826     511600
##  9  108.  31.2 达州市 city  0818     511700
## 10  106.  30.8 南充市 city  0817     511300
## # ... with 11 more rows
## 
## [[2]]
## # A tibble: 8 x 6
##     lng   lat name     level    citycode adcode
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; 
## 1  103.  36.7 永登县   district 0931     620121
## 2  104.  36.3 皋兰县   district 0931     620122
## 3  104.  36.1 西固区   district 0931     620104
## 4  103.  36.3 红古区   district 0931     620111
## 5  104.  36.1 安宁区   district 0931     620105
## 6  104.  36.0 城关区   district 0931     620102
## 7  104.  36.1 七里河区 district 0931     620103
## 8  104.  35.8 榆中县   district 0931     620123
## 
## [[3]]
## # A tibble: 11 x 6
##      lng   lat name   level    citycode adcode
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; 
##  1  116.  35.7 汶上县 district 0537     370830
##  2  117.  35.4 邹城市 district 0537     370883
##  3  117.  35.7 泗水县 district 0537     370831
##  4  117.  35.6 曲阜市 district 0537     370881
##  5  117.  35.4 任城区 district 0537     370811
##  6  116.  35.4 嘉祥县 district 0537     370829
##  7  116.  35.8 梁山县 district 0537     370832
##  8  117.  35.0 鱼台县 district 0537     370827
##  9  116.  35.1 金乡县 district 0537     370828
## 10  117.  34.8 微山县 district 0537     370826
## 11  117.  35.6 兖州区 district 0537     370812
</code></pre>
<p>注意，由于 <code>getAdmin()</code> 可以对互相独立的行政区域进行批量查询，因此默认将查询结果也许并无意义，因此需要用户自行从 list 中提取元素以便进一步使用。</p>
<h3 id="实验性功能坐标转换">实验性功能——坐标转换</h3>
<p>由于不同的地图服务，其坐标有时会加入一定程度的偏移，因此为了使坐标在高德地图中查询准确，可以首先对原始坐标进行转换，<a href="https://lbs.amap.com/api/webservice/guide/api/convert">目前高德开放平台 API 提供将 GPS 坐标、mapbar 坐标、baidu 坐标转换为高德坐标</a>。</p>
<p><strong>但是因为开发者目前并没有真正使用过这类功能，因此目前该方法尚处于实验阶段。无论是结果准确性，以及使用的便捷程度以及合理性均不作任何保证。同时在将来也许会进行 Breaking Upgrade 造成目前的调用方式不可用，因此极不推荐将该 function 引入生产环境。同时也希望大家对于该功能的完善给出建议。</strong></p>
<pre><code class="language-r">convertCoord(&quot;116.481499,39.990475&quot;, coordsys = &quot;gps&quot;)
</code></pre>
<pre><code>## # A tibble: 1 x 2
##     lng   lat
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  116.  40.0
</code></pre>
<p>更多功能以及改进还在开发中。</p>
<h2 id="常见问题">常见问题</h2>
<h3 id="会引入并行请求吗">会引入并行请求吗？</h3>
<p>虽然在开发之初，笔者认为将来不会加入并行，不过现在的计划，大概是会吧，谁知道呢？ <s>Unfortunately, there is no plan to add internal parallel support to amapGeocode. Here are some reasons:</s></p>
<p><s>1. The aim of amapGeocode is to create a package which is easy to use. Indeed, the parallel operation can make many times performance improvement, especially there are half million queries. However, the parallel operation often platform limited, I don’t have enough time and machine to test on different platforms. In fact even in macOS, the system I’m relatively familiar with, I have already encountered a lot of weird parallel issues and I don’t have the intention or the experience to fix them.</s></p>
<p><s>2. The queries limitation. For most of free users or developers, the daily query limitation and queries per second is absolutely enough: 30,000 queries per day and 200 queries per second. But for parallel operation, the limitation is relatively easy to exceed. For purchase developers, it may cause serious financial troubles.</s></p>
<p><s>So for anybody who wants to send millions of request by amapGeocode, you are welcomed to make the parallel operations manually.</s></p>
<h2 id="bug-report">Bug Report</h2>
<p>对于 API Wrap 类型的包，上游 API 更新造成的功能不可用在所难免。如果你遇到这类故障或者任何其它 Bug，请第一时间让我知道 <a class="github-button" href="https://github.com/womeimingzi11/amapGeocode/issues" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" aria-label="Issue womeimingzi11/amapGeocode on GitHub">Issue</a>！</p>
<p>如果 amapGeocode 对你有任何帮助，希望可以帮我 <a class="github-button" href="https://github.com/womeimingzi11/amapGeocode" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" aria-label="Star womeimingzi11/amapGeocode on GitHub">Star</a></p>
<hr>
<p>欢迎通过<a href="mailto://chenhan28@gmail.com">邮箱</a>，<a href="https://weibo.com/womeimingzi11">微博</a>, <a href="https://twitter.com/chenhan1992">Twitter</a>以及<a href="https://www.zhihu.com/people/womeimingzi">知乎</a>与我联系。也欢迎关注<a href="https://https://blog.washman.top/">我的博客</a>。如果能对<a href="https://github.com/womeimingzi11">我的 Github</a> 感兴趣，就再欢迎不过啦！</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[R for Everything 番外: R 包开发中 vignette/test token 的处理]]></title>
        <id>https://womeimingzi11.github.io/post/mEmVc9Hsx/</id>
        <link href="https://womeimingzi11.github.io/post/mEmVc9Hsx/">
        </link>
        <updated>2020-09-24T20:51:38.000Z</updated>
        <summary type="html"><![CDATA[<p>skip_on_cran() for testhat and NOT_CRAN for vignette</p>
]]></summary>
        <content type="html"><![CDATA[<p>skip_on_cran() for testhat and NOT_CRAN for vignette</p>
<!-- more -->
<p>作为一个不怎么 Skr 的 API Wrapper，最近花式的用到了高德地图的地理编码/逆编码服务，与其散乱各处每次都要反复修改 function 代码，不如干脆就封装成 package，还能方便自己和团队今后的使用。</p>
<p>作为年轻人（并不）的第一个 Package，当然是参考 Hadley 的经典手册<a href="https://r-pkgs.org/">《R Package》</a>，此外还参考了来自 Fong Chun Chan’s Blog 的 <a href="https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html">Making Your First R Package</a> 作为快速手册。Functions 编写的过程非常的愉快，毕竟只是 url 与 reques body 的组装，在 <a href="https://CRAN.R-project.org/package=httr">httr</a>, <a href="https://arxiv.org/abs/1403.2805">jsonlite</a> 以及 <a href="https://CRAN.R-project.org/package=xml2">XML2</a> 等三个主要包的帮助下，API Wrap 只不过是体力活。目前包正在 CRAN 审核，如果上线了应该又能水一篇文章，不过没上线也应该会水一篇……记账上吧先。</p>
<p>然而，与写业务代码相比，提交包到 CRAN 没想到是最痛苦的。最主要的原因之一，就是作为一个 API 调用的包，几乎所有的操作都需要调用 token：</p>
<ol>
<li>写 function，需要提供 key 参数（<em>高德提供 token，官方文档称为 key，为了统一，一并称为key，下同</em>），这个很简单，单独赋值或者从 <code>getOption('amap_key')</code> 中获取（<em>此为 function 中的逻辑，非 R 缺省操作，下同</em>）均可，无论是测试还实际业务都能轻松完成，也无需暴露 key；</li>
<li>写 README，虽然不是 CRAN 强制要求的，但是作为肯定会在 GITHUB 开源的项目，README 必不可少，从 Rmarkdown 到 Markdown，因为是在本地编译，也可以从 Option 中获取，因此编译成的 Markdown 和原始 Rmarkdown 文件中均可隐去 key；</li>
<li>写单元测试，这个就开始有坑了。虽然<code>devtools::test</code>可以通过测试，但是在<code>devtools::check()</code>中因为每次测试都是在独立的 NAMESPACE 中进行的，因此即便是设置了 <code>option(var1 = value1)</code>，在测试中的 <code>getOption('var1')</code> 依然返回 <code>NULL</code>，因此需要在测试中加入 key 的值，这就使得 token 会随着 package 的分发，对所有的用户可见；</li>
<li>写 vignette，这个坑和单元测试情况相似，即便是 option 中加入了变量，依然不能保证可以正确编译，并且比较玄学的是某些情况下，可以从 option 中读取变量，但是在 R Studio 的 Build 页面中运行 check 又不可以获取，最终报错。而且即便是本地编译成功，如果不做任何额外的操作，发送到远端服务器后依然无法编译成功，原因也很简单对方的环境中毕竟没有 key 变量，因此需要在 vignette 相关部分加入 key 的值，再次会造成 key 的泄露。</li>
</ol>
<p>虽然，作为免费的 token，大概有心盗用的人也不会太多，况且 package 有人用没人用还是很难乐观的议题。然而，信息泄露总归是不好的。</p>
<p>为了解决问题 3、4，花费了比写业务代码还久的时间，虽然有很大的概率不是最优，并且其实书里面有提到。但既然自己遇到了，并且<br>
Stackoverflow 也有人提完，不如依然做个记录，万一能帮到同样没认真看书的朋友呢?</p>
<h2 id="no-vignette-no-test-no-trouble">No vignette, No test, No Trouble?</h2>
<p>最先想到的方法，当然是放弃。毕竟我们平日写 function，连注释都没有，写一个不怎么样的包，怎么还要写文档了呢?自己去悟不好么?</p>
<p>关于 vignette 和 test 的必要性，这里就不再做科普了，感兴趣的朋友可以参考《R Package》中的章节 <a href="https://r-pkgs.org/vignettes.html"><em>Vignettes: long-form documentation</em></a> 以及 <a href="https://r-pkgs.org/tests.html"><em>Testing</em></a>。然而其实书里面<strong>好像是</strong>没讲到的问题是，远程检查代码的过程中会回报一个 Warning：<br>
<img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Fig01_no_e_no_t_no_v.png" alt="" loading="lazy"></p>
<p>诚然，Warning ~= Can’t be better，然而在 CRAN 提交的过程中，WARNING ~= ERROR</p>
<blockquote>
<p>Check results: I always state that there were no errors or warnings. Any NOTEs go in a bulleted list. For each NOTE, I include the message from R CMD check and a brief description of why I think it's OK. If there were no NOTEs, I'd say “There were no ERRORs, WARNINGs or NOTEs —— R Package: Chapter 20.3 The submission process”</p>
</blockquote>
<p>虽然可以选择 Upload 到 CRAN，不过在 Review 步骤也很容易被返回。因此这种豁达的操作是行不通的。</p>
<h2 id="skip_on_cran-for-testthat">skip_on_cran() for testthat</h2>
<p>在 test 部分想要解决这个问题，其实可以在 test 起始部分加入 skip_on_cran() 方法（下列代码第二行）</p>
<pre><code class="language-r"># Test whether getAdmin can retrun right class withou to_tibble
test_that(&quot;Reuturn raw respone with correct location&quot;, {
    skip_on_cran()
    res &lt;- getAdmin(&quot;四川省&quot;, to_table = F)
    res_class &lt;- class(res)
    
    expect_equal(any(stringr::str_detect(res_class, &quot;list&quot;)), TRUE)
})
</code></pre>
<p>这个方法其实在 R Package: Chapter 12.5 CRAN notes 有提到：</p>
<blockquote>
<p>Tests need to run relatively quickly - aim for under a minute. Place  skip_on_cran() at the beginning of long-running tests that shouldn't be run on CRAN - they'll still be run locally, but not on CRAN.</p>
</blockquote>
<p>不过一方面是因为看书不认真，另一方面只是注意到这方法 Hadley 的意图是加快测试时间，只在本地进行测试，因此忽略了这种无法在远端进行测试的场景。</p>
<p>不过添加这个方法后问题也不是全然解决，因为正如前面所说，<code>devtools::check</code>中的单元测试会在单独的 NAMESPACE 中进行，因此即便是在本地运行，如果不单独指定 key，检查过程依然会失败。因此我们用到了<code>skip()</code>family 的另外一种形式<code>skip_if()</code>来进行了条件判断:即只有满足 condition 的时候才会进行单元测试，在本案例中具体而言就是 option 不存在 amap_key 参数，则不进行测试：</p>
<pre><code class="language-r">skip_if(is.null(getOption(&quot;amap_key&quot;)))
</code></pre>
<p>更多 skip 方法请参考<code>help(testthat::skip)</code></p>
<p>no test 的 Warning 被解决，只不过因为 CRAN 会跳过测试步骤，处于保险起见，还是应该在本地进行更仔细的测试。</p>
<h2 id="not_cran-for-vignette">NOT_CRAN for vignette</h2>
<p><strong>Note: 这个部分，虽然解决了 Warning 警告，但实际上处理并不完美：无法在文档中预览变量，实际上只是显示了静态代码块。如果有更好的解决方案，还望不吝赐教</strong></p>
<p>在 Hadley 为 httr 编写的 vignette <a href="https://cran.r-project.org/web/packages/httr/vignettes/secrets.html">Managing secrets</a> 中对于如何处理保密的数据有了很充分的讨论。</p>
<blockquote>
<p>Vignettes pose additional challenges when an API requires authentication, because you don't want to bundle your own credentials with the package! However, you can take advantage of the fact that the vignette is built locally, and only checked by CRAN.</p>
</blockquote>
<p>因为 Vignettes 是在本地进行编译的，因此我们可以指定仅仅使用 CRAN 进行检查工作（而非再次构建）。</p>
<p>只要在首次调用 token 的代码块之前添加代码块设定 <code>knitr:opts_chunk$set</code>，通常是 vignette 文字开始之前，文档的 YAML Header 结束之后。</p>
<pre><code class="language-r">NOT_CRAN &lt;- identical(tolower(Sys.getenv(&quot;NOT_CRAN&quot;)), &quot;true&quot;),
knitr::opts_chunk$set(purl = NOT_CRAN)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Fig02_chunk_set.png" alt="" loading="lazy"></figure>
<p>之后在所有需要调用 Token 的代码块中，将 eval 设定为 <code>eval = NOT_CRAN</code>，之后在 CRAN 检查中 vignette 构建的步骤便不会报 ERROR。</p>
<p><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Fig03_eval_NOT_CRAN.png" alt="" loading="lazy"><br>
<em>图中有重复赋值，已修改</em></p>
<p>在该方法中，会对 R 运行环境进行检查，如果当前环境为 CRAN 则不运行 vignette 文档中的代码块，也正因为如此，Rmarkdown 中的代码块也就不会返回执行内容，因此文档中也不会显示结果，代码块仅做静态展示之用。</p>
<figure data-type="image" tabindex="2"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Fig04_no_preview.png" alt="" loading="lazy"></figure>
<h2 id="错误的挣扎">错误的挣扎</h2>
<p>在第二张图中，可以发现在使用 NOT_CRAN 来解决 vignette 错误之前，我还常使用 source R script 的方法解决问题，当时的设想很完美：将 <code>option(amap_key=&quot;My token&quot;)</code>写入外部文件，之后将外部文件放入 .Rbuildignore 中在 package 打包的过程中排除掉，是否就能得到完美的解决方案了呢?</p>
<p>然而实际上，放入 .Rbuildignore 中的文件因为被排除在了打包之外，而构建 vignette 是完成打包之后方才进行，因此这种情况下是找不到文件地址的（因为没有在已经打包完成的library目录中）。</p>
<p>通过这些方法，目前暂时实现了 0 error 0 warning，各位处理过相似问题的前辈如果有更合理的解决方案，还望能分享指点。</p>
<hr>
<p>欢迎通过<a href="mailto://chenhan28@gmail.com">邮箱</a>，<a href="https://weibo.com/womeimingzi11">微博</a>, <a href="https://twitter.com/chenhan1992">Twitter</a>以及<a href="https://www.zhihu.com/people/womeimingzi">知乎</a>与我联系。如果能对<a href="https://github.com/womeimingzi11">我的 Github</a> 感兴趣，就再欢迎不过啦！</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shiny apps: 使用 moreThanANOVA 进行「正确」的显著性检验]]></title>
        <id>https://womeimingzi11.github.io/post/YdOGNuodh/</id>
        <link href="https://womeimingzi11.github.io/post/YdOGNuodh/">
        </link>
        <updated>2020-06-09T06:41:26.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="tldr">TL:DR</h2>
<p>使用 <a href="https://hanchen.shinyapps.io/moreThanANOVA/">moreThanANOVA</a> 可以自动选择适当的方法显著性检验并输出多重比较的图表（如题图）</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="tldr">TL:DR</h2>
<p>使用 <a href="https://hanchen.shinyapps.io/moreThanANOVA/">moreThanANOVA</a> 可以自动选择适当的方法显著性检验并输出多重比较的图表（如题图）</p>
<!-- more -->
<h2 id="anova-不是万金油">ANOVA 不是万金油</h2>
<p>显著性检验可以说是自然科学领域中最常见的操作，毕竟探究处理间是否有差异、区域间特征异同是后续探索以及假设的第一步。</p>
<p>然而，当我们阅读文献和报告之时不难发现，单因素方差分析 ANOVA 似乎是被用来进行显著性检验的唯一方法，但凡展示显著性，如果没什么意外必然是通过 ANOVA 做出的结果。而之所以出现这种状况，除了数据确实需要 ANOVA 检验，「其他人都用这个方法」也是重要因素。然而，用的人多的方法就一定是对的方法吗？</p>
<p><strong>当然不是</strong>，使用 ANOVA 检验数据最重要的前提便是假设数据符合正态分布。</p>
<p>虽然通常来说，当数据的量足够大的情况下，往往会呈现出正态分布，但当具体到某次研究中，结果却并不见得如此。当然我们还有数据转换这类操作，可以试图让各种数据尽可能的达到符合正态分布，然而就像不是所有的付出都有回报一样，也不是所有的数据转换都能达到正态期望，亦或者通过复杂的转换数据也许能够实现正态分布，然而复杂的转换本身又会令解释工作变得复杂。</p>
<p>当然，即便数据分布不符合正态分布，使用 ANOVA 进行显著性检验也会得到结果，甚至是你喜欢的结果。然而使用错误的方法得到的「美好」结果依然是错误的。</p>
<h2 id="不止是-anova">不止是 ANOVA</h2>
<p>对于无法使用 ANOVA 进行显著性检验的数据，我们可以退而求其次选择<strong>非参数检验</strong> non-parametric tests，其中最常见的便是秩检验 Rank test。对于只有两组/处理的数据，通常使用 <strong>Mann–Whitney U test</strong>或<strong>Wilcox test</strong>，对于有三组/处理及以上的数据可以使用 <strong>Kruskal-Wallis test</strong>.</p>
<p>此外，根据组间样本数量异同还分为秩和检验 Rank Sum Test 和秩序检验 Signed Rank Test，其中的细节此处不再详述，可以参考如下文章：</p>
<ol>
<li><a href="http://www.sthda.com/english/wiki/unpaired-two-samples-wilcoxon-test-in-r">Unpaired Two-Samples Wilcoxon Test in R</a></li>
<li><a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test">wilcox.test</a></li>
</ol>
<p>虽然非参数检验其数据敏感度与 ANOVA 这类参数检验数据敏感度相比较低，然而正确的分析方法依然是所有讨论的前提。</p>
<h2 id="正确的流程">正确的流程</h2>
<p>因此，一次正确的显著性检验的流程应当为：</p>
<ol>
<li>输入数据</li>
<li>数据分布探索</li>
<li>数据转换（可选）</li>
<li>检验方法确定</li>
<li>显著性检验</li>
<li>多重比较</li>
</ol>
<p>虽然这些步骤每一步使用 R 实现都并不复杂，然而当需要比较组间 N 个变量的差异时，即便采用了 map-reduce 模式处理，依然需要大量的工作，更何况如何将 R console 中打印的结果整理为可以编辑的表格以及将结果制作成可供出版的示意图，也会需要更多额外的工作。</p>
<p>基于此，笔者使用 Shiny 将这套流程打包成名为 <a href="https://hanchen.shinyapps.io/moreThanANOVA">moreThanANOVA 的 Shiny App </a> 托管于 Shinyapps.io.</p>
<h2 id="it-just-works">It just works</h2>
<h3 id="data-viewer">Data Viewer</h3>
<p>在 Data Viewer 页面，通过 DT 包实现了展示输入数据的功能，当没有上传数据之时，可以通过 Data Viewer 查看 moreThanANOVA 内置的示例数据（<em>数据本身无意义，仅供演示之用途</em>）。</p>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Data_Viewer_2" alt="Data Viewer" loading="lazy"></figure>
<h3 id="distribution-determine">Distribution Determine</h3>
<p>在 Distribution Determine 页面，moreThanANOVA 会检验所有变量在不同组中的分布类型，目前仅分为 normal 和 skewed，然而对于双峰分布等其他分布类型，非参数检验也依然适用，因此之后的分析结果依然可靠。</p>
<figure data-type="image" tabindex="2"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/data_dist" alt="" loading="lazy"></figure>
<p>当同一变量在不同组中的分布类型不相同时，moreThanANOVA 会采用非参数检验进行之后的分析，同时在 Density Plot 中绘制数据分布密度图。</p>
<figure data-type="image" tabindex="3"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/density_plot" alt="" loading="lazy"></figure>
<h3 id="comparisons">Comparisons</h3>
<p>在 Comparisons 页面，首先 moreThanANOVA 会通过在前期步骤中确定的检验方法，进行组间显著性检验。同时会将均值、标准差、中位数以及四分位距 Interquartile range, IQR 输出为可下载的可交互表格。</p>
<figure data-type="image" tabindex="4"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/compare_table" alt="" loading="lazy"></figure>
<p>最后，moreThanANOVA 还会生成基于多重检验的图形，目前可以自定义图中 X/Y 轴以及题头标签，显示相关性级别、相关性的表示方法（p 或者 *）、多组图片之间的排列方式以及下载图片的宽、高。</p>
<p>####为何采用 PDF 输出图形？<br>
使用 PDF 可以输出矢量图，能够有效避免 DPI 造成的清晰度问题，同时拥有极小的体积以及不错的兼容性。</p>
<figure data-type="image" tabindex="5"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/post_hoc_figure.png" alt="" loading="lazy"></figure>
<p>目前 moreThanANOVA 托管于 Rstudio 旗下 Shinyapps.io 网站，源代码托管于 <a href="https://github.com/womeimingzi11/moreThanANOVA">GitHub</a>。服务运行过程中不会永久保存用户数据，数据有效期仅保留至网页关闭前。</p>
<p>同时如果对于数据高度敏感，也可以考虑将<a href="https://github.com/womeimingzi11/moreThanANOVA/fork/"> fork 项目</a> 或下载至本地后，手动运行 app.R 在本地电脑执行操作。</p>
<p>如有任何使用中的疑问、功能上的建议与意见，欢迎与<a href="mailto://chenhan28@gmail.com">我联系</a>。当然也欢迎 <a href="https://github.com/womeimingzi11/moreThanANOVA/star/">Star</a> 以及进行任何提交。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[R for everything: 方差分解（Variation partition）变量筛选与显著性标注]]></title>
        <id>https://womeimingzi11.github.io/post/HpSw5NiHj/</id>
        <link href="https://womeimingzi11.github.io/post/HpSw5NiHj/">
        </link>
        <updated>2020-05-11T08:21:57.000Z</updated>
        <content type="html"><![CDATA[<h2 id="何为方差分解">何为方差分解？</h2>
<p>关于方差分解的数学原理，在 GUSTA ME 这篇文章 <strong><a href="https://mb3is.megx.net/gustame/constrained-analyses/variation-partitioning">Variation partitioning</a></strong> 有非常清晰且容易理解的介绍。</p>
<p>引用经管之家网友 <a href="https://bbs.pinggu.org/home.php?mod=space&amp;uid=1801145">lovecather668</a> 在**<a href="https://bbs.pinggu.org/thread-1125443-1-1.html">var中方差分解的意义</a>** 中的回答：</p>
<blockquote>
<p>VAR中的方差分解是分析影响内生变量的结构冲击的贡献度。例如，有好多行业产品的需求变动会对钢铁行业产品的需求变动产生影响，像建材行业、汽车行业、机械行业、家电行业。那么如果我们想要知道这4个行业的需求变化对钢铁行业的需求变化产生的影响哪个大、哪个小呢，就可以用方差分解来做。做出来的结果是用贡献率（百分比）来表示的，如假设结果是以上4个行业在某个时点上的贡献率分别为10%，12%，16%，20%（随时间的变化，这个贡献率也是在变化的），其意思是在该时点钢铁行业需求的变动，10%是建材行业的需求变动引起的，12%是汽车行业的需求变动引起的，以此类推.....</p>
</blockquote>
<p>在生命科学的实际应用中，可以基于 RDA 结果得出不同类型的环境因素（如何气候、土壤性质以及植物）对生物群落组成（如土壤线虫群落、微生物群落）的解释程度。</p>
<p>Singh et al.(2018) 在 Scientific Report 上发表的文章<a href="https://doi.org/10.1038/s41598-019-42333-4">《tropical forest conversion to rubber plantation affects soil micro- &amp; mesofaunal community &amp; diversity》</a>中提到了一套完整的基于 RDA 的方差分解操作：</p>
<blockquote>
<p>We performed a redundancy analysis (RDA) based variation partitioning analysis 57 to assess the relative effects of environmental and spatial variables on community composition. We used Hellinger transformed OTU abundance data as the response variable and two sets of explanatory variables which included environmental variables (pH, Ele, TOC, AP, GSW, SX, TN, and ST) and spatial variables (geographical co-ordinates for sampling sites), respectively. Before the RDA, the environmental variables with high variance inflation factor (VIF) &gt;10 were eliminated to avoid collinearity among factors. The importance of environmental and spatial variables in explaining species composition was determined by an RDA analysis using Monte Carlo permutation tests (999 unrestricted permutations) followed by forward selection to remove the non-significant variables from each of the explanatory sets.</p>
</blockquote>
<p>如何进行这样的方差分解流程（包括变量筛选以及变量集的显著性标注）就是今天这篇文章的重点。</p>
<h2 id="使用-r-进行方差分解">使用 R 进行方差分解</h2>
<p>正如大多数基于群落的分析方法相同，在 R 中进行方差分解同样离不开 <a href="https://cran.r-project.org/web/packages/vegan/vegan.pdf">vegan 包</a>，主要依赖两个功能：</p>
<ol>
<li><a href="https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/ordistep">ordistep</a> - 变量筛选</li>
<li><a href="https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/varpart">varpart</a> - 方差分解</li>
</ol>
<p>数据继续使用之前的文章<a href="https://womeimingzi11.github.io/post/vY312ek1B/">《使用 ggplot2 可视化 RDA 结果》</a>中使用的演示数据。</p>
<pre><code class="language-R"># suppressMessages will hide message from loading package, it makes the output clearer
suppressMessages(library(tidyverse))
# col_types = cols() will suppress message about the column type to make the output clearer
df_com &lt;- read_csv('df_com_smp.csv', col_types = cols())
df_env &lt;- read_csv('df_env_smp.csv', col_types = cols())

# Take a glimpse about the structure of data frame
glimpse(df_com)
glimpse(df_env)

</code></pre>
<pre><code>Rows: 150
Columns: 5
$ pp [3m[38;5;246m&lt;dbl&gt;[39m[23m 2.5500000, 1.3110456, 8.2500000, 102.4900000, 21.3500000, 2.490000…
$ ba [3m[38;5;246m&lt;dbl&gt;[39m[23m 28.010000, 7.316707, 13.255000, 18.516667, 52.720000, 4.965000, 61…
$ fu [3m[38;5;246m&lt;dbl&gt;[39m[23m 2.5500000, 1.3378838, 3.3550000, 20.9466667, 14.8466667, 1.2450000…
$ pr [3m[38;5;246m&lt;dbl&gt;[39m[23m 2.5500000, 1.3378838, 0.0000000, 0.0000000, 0.0000000, 0.0000000, …
$ om [3m[38;5;246m&lt;dbl&gt;[39m[23m 15.280000, 5.953381, 14.850000, 23.500000, 5.623333, 2.475000, 31.…
Rows: 150
Columns: 13
$ pH  [3m[38;5;246m&lt;dbl&gt;[39m[23m -1.63965465, 0.94291257, -1.33875709, -1.20359469, -1.17624072, 0…
$ MOI [3m[38;5;246m&lt;dbl&gt;[39m[23m -0.1610384, -0.5549069, -0.2762522, -0.1658411, 1.0043821, 0.5467…
$ TN  [3m[38;5;246m&lt;dbl&gt;[39m[23m -0.116240631, -0.791972999, 0.199881689, -0.260084650, 0.29187495…
$ TP  [3m[38;5;246m&lt;dbl&gt;[39m[23m -0.44892589, 0.02952209, -0.14900328, -0.45606691, -0.47748995, -…
$ AP  [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.17966965, -0.78701735, -0.10273314, 0.09277663, 2.24338416, 0.5…
$ NH4 [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.4471754, -0.3426843, 0.7179844, -0.1328073, 0.7924569, -0.36525…
$ NO3 [3m[38;5;246m&lt;dbl&gt;[39m[23m -0.51485153, -0.41977560, -0.49221441, -0.16774892, -0.58276291, …
$ SOC [3m[38;5;246m&lt;dbl&gt;[39m[23m -0.292278803, -0.697789311, 0.461059956, -0.007572046, 0.03580922…
$ MAT [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.84202205, -1.55196703, -0.84801694, 0.78335952, -0.28932636, -0…
$ MAP [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.02487222, 0.27013035, 1.21243154, -0.87351309, -0.22471654, 1.2…
$ PBM [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.17638650, -0.02465146, 0.73829039, -0.80968336, 0.12645186, 2.1…
$ PCV [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.426595458, 0.008333099, -0.793336422, -0.514494850, 0.182609082…
$ PSR [3m[38;5;246m&lt;dbl&gt;[39m[23m 0.22757812, -0.83529465, 0.83493399, -1.29081156, 0.98677296, 0.6…
</code></pre>
<p>按照 varpart 在文档中的说明：</p>
<blockquote>
<p>The functions partition the variation in Y into components accounted for by two to four explanatory tables and their combined effects. If Y is a multicolumn data frame or matrix, the partitioning is based on redundancy analysis (RDA, see rda), and if Y is a single variable, the partitioning is based on linear regression. If Y are dissimilarities, the decomposition is based on distance-based redundancy analysis (db-RDA, see capscale) following McArdle &amp; Anderson (2001). The input dissimilarities must be compatible to the results of dist. Vegan functions vegdist, designdist, raupcrick and betadiver produce such objects, as do many other dissimilarity functions in R packages. However, symmetric square matrices are not recognized as dissimilarities but must be transformed with as.dist. Partitioning will be made to squared dissimilarities analogously to using variance with rectangular data -- unless sqrt.dist = TRUE was specified.</p>
</blockquote>
<p>对于群落矩阵，方差分解是按照 RDA 结果来进行处理的，因此筛选变量的流程可在 RDA 分析的过程中完成。</p>
<h3 id="为何要筛选变量">为何要筛选变量？</h3>
<p>在多因素的分析中，例如 RDA 分析和多元线性回归，任何增加新的变量，都会使得模型「看起来」拟合得更好，然而实际情况显然并非如此，为了解决过拟合问题<strong>调整 R 方</strong>是一个很常用的指标，具体的解释可以查看 <a href="https://www.statisticshowto.com/adjusted-r2/">Adjusted R2 / Adjusted R-Squared: What is it used for?</a>。</p>
<p>回到正题，对于 RDA 分析选择变量，ordistep 提供了三种方法 backward, forward, both，即从全模型逐渐消除变量、从零模型逐渐增加变量以及双向选择变量。这里我们同时进行三种变量选择的方式。</p>
<pre><code class="language-R"># suppressMessages will hide message from loading package, it makes the output clearer
suppressMessages(library(vegan))

# Create the full model
rda_full &lt;- rda(df_com~., data = df_env)

# Create the zero model
rda_null &lt;- rda(df_com~1, data = df_env)

# backward selection
# trace = 0 prevent ordistep print the selection progress from outputing to the console
# it makes the output clearer.
rda_back &lt;- ordistep(rda_full, direction = 'backward',trace = 0)

# forward selection
rda_frwd &lt;- ordistep(rda_null, formula(rda_full), direction = 'forward',trace = 0)

# bothward selection 
rda_both &lt;- ordistep(rda_null, formula(rda_full), direction = 'both',trace = 0)

rda_back
rda_frwd
rda_both
</code></pre>
<pre><code>Call: rda(formula = df_com ~ pH + MOI + TN + TP + NO3 + SOC + MAT, data
= df_env)

                Inertia Proportion Rank
Total         2031.7341     1.0000     
Constrained    860.1782     0.4234    5
Unconstrained 1171.5559     0.5766    5
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2  RDA3  RDA4  RDA5 
747.1  77.3  32.2   2.5   1.0 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5 
755.6 247.3 108.6  46.6  13.5 




Call: rda(formula = df_com ~ MOI + NO3 + TN + SOC, data = df_env)

                Inertia Proportion Rank
Total         2031.7341     1.0000     
Constrained    790.2092     0.3889    4
Unconstrained 1241.5248     0.6111    5
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2  RDA3  RDA4 
714.8  70.1   4.3   1.0 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5 
787.6 277.0 114.1  48.7  14.0 




Call: rda(formula = df_com ~ MOI + NO3 + TN + SOC, data = df_env)

                Inertia Proportion Rank
Total         2031.7341     1.0000     
Constrained    790.2092     0.3889    4
Unconstrained 1241.5248     0.6111    5
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2  RDA3  RDA4 
714.8  70.1   4.3   1.0 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5 
787.6 277.0 114.1  48.7  14.0 
</code></pre>
<p>根据实际情况选择最终的模型，这里我们选择向后消除法选择的模型作为 RDA 最终的结果。同时这也是方差分解的最终模型，即 <code>df_com ~ pH + MOI + TN + TP + NO3 + SOC + MAT</code>，其中 MAT 是气候因素，我们将其选入气候因素集 df_clim， 其余均为土壤性质因素集 df_soil，植物因素没有选入模型。</p>
<pre><code class="language-R"># Divide the environmental data.frame by different categories.
df_clim &lt;- df_env %&gt;% select(MAT)
df_soil &lt;- df_env %&gt;% select(pH,MOI,TN,TP,NO3,SOC)

# Perform variation partitioning analysis, the first variable is the community matrix
# the second and third variables are climate variable set and soil property variable set
vpt &lt;- varpart(df_com, df_clim, df_soil)

vpt

plot(
    vpt,
    bg = 2:5,
    id.size = 1.1,
    cex = 1.2,
    Xnames = c('Climate', 'Soil properties')
)
title('Variation partitioning by varpart from vegan')
</code></pre>
<pre><code>Partition of variance in RDA 

Call: varpart(Y = df_com, X = df_clim, df_soil)

Explanatory tables:
X1:  df_clim
X2:  df_soil 

No. of explanatory tables: 2 
Total variation (SS): 302728 
            Variance: 2031.7 
No. of observations: 150 

Partition table:
                     Df R.squared Adj.R.squared Testable
[a+b] = X1            1   0.00855       0.00186     TRUE
[b+c] = X2            6   0.41263       0.38799     TRUE
[a+b+c] = X1+X2       7   0.42337       0.39495     TRUE
Individual fractions                                    
[a] = X1|X2           1                 0.00696     TRUE
[b]                   0                -0.00510    FALSE
[c] = X2|X1           6                 0.39309     TRUE
[d] = Residuals                         0.60505    FALSE
---
Use function ‘rda’ to test significance of fractions of interest
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/output_5_1.png" alt="" loading="lazy"></figure>
<p>根据 RDA 和方差分解的结果（实际上是相同的结果），我们可以发现气候因素与土壤性质共同解释了 39.5% 的群落结构变异，其中土壤性质解释了 38.8% 的群落组成变异，气候解释了 0.2% 的群落组成变异。然而对于二者是否显著，还需要进一步检验。</p>
<pre><code class="language-R"># Define formula of soil property set and climate set to test.
# Set the variable from other category as condition
formula_soil &lt;- formula(df_com ~ pH+MOI+TN+TP+NO3+SOC + Condition(MAT))
formula_clim &lt;- formula(df_com ~ Condition(pH)+Condition(MOI)+Condition(TN)+Condition(TP)+Condition(NO3)+Condition(SOC) + MAT)

anova(rda(formula_soil, data = df_env))
anova(rda(formula_clim, data = df_env))
</code></pre>
<table>
<caption>A anova.cca: 2 × 4</caption>
<thead>
	<tr><th></th><th scope=col>Df</th><th scope=col>Variance</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>Model</th><td>  6</td><td> 842.7984</td><td>17.02542</td><td>0.001</td></tr>
	<tr><th scope=row>Residual</th><td>142</td><td>1171.5559</td><td>      NA</td><td>   NA</td></tr>
</tbody>
</table>
<table>
<caption>A anova.cca: 2 × 4</caption>
<thead>
	<tr><th></th><th scope=col>Df</th><th scope=col>Variance</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>Model</th><td>  1</td><td>  21.81623</td><td>2.644266</td><td>0.083</td></tr>
	<tr><th scope=row>Residual</th><td>142</td><td>1171.55586</td><td>      NA</td><td>   NA</td></tr>
</tbody>
</table>
<p>结合单因素方差分析 ANOVA 得到的结果，我们可以得出最终结论，土壤性质能够显著解释 38.8% 的群落组成变异，气候因素对于群落结构变异的解释不显著。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[R for Everything: 使用 ggplot2 可视化 RDA 结果]]></title>
        <id>https://womeimingzi11.github.io/post/vY312ek1B/</id>
        <link href="https://womeimingzi11.github.io/post/vY312ek1B/">
        </link>
        <updated>2020-04-28T09:05:16.000Z</updated>
        <content type="html"><![CDATA[<h1 id="using-r-for-everything-ggplot2-可视化-rda-结果">Using R for Everything: ggplot2 可视化 RDA 结果</h1>
<p>关于 RDA（Redundancy analysis 冗余分析）是什么，相信对于已经有可视化需求的同学来说已经不用更多的解释了。</p>
<p>在 R 中常用来进行 RDA 分析和绘制工作的是 <a href="https://cran.r-project.org/web/packages/vegan/vegan.pdf">vegan</a> 和 <a href="https://github.com/gavinsimpson/ggvegan">ggvegan</a> 这两个包。然而，在实际使用中，最常遇到的问题是虽然这些包内建的 plot 等功能可以绘制出基本可用的包，但想要进一步的定制图形却没那么容易。</p>
<p>想要绘制出一副自己满意、编辑满意、导师满意最主要的是审稿人满意的 RDA 结果，作为最强可视化工具之一的 <a href="https://ggplot2.tidyverse.org/">ggplot2</a> 包毋庸置疑是最佳的选择。</p>
<h2 id="我们需要什么样的-rda-图">我们需要什么样的 RDA 图</h2>
<p>首先，我们来思考我们需要什么样的 RDA 图？按照世纪需求以及审稿人的建议：</p>
<blockquote>
<p>I would recommend showing in bold the variables with significant correlations</p>
</blockquote>
<p>笔者最后的目标是绘制一幅：</p>
<pre><code>1. 显示物种信息（实际上是响应变量矩阵）；
2. 环境变量（实际上是解释变量矩阵）；
3. 在两轴上能显示各自的解释度；
4. 标记有显著性的解释变量。
</code></pre>
<p>以笔者对 plot.rda 以及 autoplot.rda 这两个 vegan 和 ggvegan 内建函数的浅薄了解，似乎很难完成。</p>
<h3 id="如何标记有显著性的解释变量">如何「标记有显著性的解释变量」？</h3>
<p>如果进行过 RDA 分析不难发现，使用 vegan 内建的 rda 是没有标记解释变量的显著性的。其实这种显著性需要 vegan 内建的 envfit 函数来得到一个及其近似的结果。通常这个方法在论文中会写作：</p>
<blockquote>
<p>Monte Carlo permutation (999 permutations) was used to identify axes with significant eigenvalues and species-environment correlations.</p>
</blockquote>
<p>由于是进行了 999 次（默认参数可以修改）的蒙特卡洛抽样，因此这个结果是<strong>及其近似</strong>的结果，直接用作 RDA 中解释变量的显著性是没有问题的。然而，envfit 输出的结果并非标准的 data.frame 或者类似的结果，无法方便的输出或者进行分析，后续我们还会进行结果提取的步骤，暂且按下不表。</p>
<h2 id="rda-和-envfit-分析">RDA 和 ENVFIT 分析</h2>
<p>此例，我们使用随机抽取的 150 条土壤线虫群落群落数据以及对应的环境数据，由于不影响后续的理解以及版权考量，对各参数名不再解释。</p>
<p>每一步的操作以及原因以注释的形式呈现。</p>
<pre><code class="language-R">library('tidyverse')

# read Environmental Variables
df_env &lt;- read_csv('df_env_smp.csv')
# read Community composition matrix
df_com &lt;- read_csv('df_com_smp.csv')

# View the structure of data
head(df_env)
head(df_com)
</code></pre>
<pre><code>Parsed with column specification:
cols(
  pH = [32mcol_double()[39m,
  MOI = [32mcol_double()[39m,
  TN = [32mcol_double()[39m,
  TP = [32mcol_double()[39m,
  AP = [32mcol_double()[39m,
  NH4 = [32mcol_double()[39m,
  NO3 = [32mcol_double()[39m,
  SOC = [32mcol_double()[39m,
  MAT = [32mcol_double()[39m,
  MAP = [32mcol_double()[39m,
  PBM = [32mcol_double()[39m,
  PCV = [32mcol_double()[39m,
  PSR = [32mcol_double()[39m
)

Parsed with column specification:
cols(
  pp = [32mcol_double()[39m,
  ba = [32mcol_double()[39m,
  fu = [32mcol_double()[39m,
  pr = [32mcol_double()[39m,
  om = [32mcol_double()[39m
)
</code></pre>
<table>
<caption>A tibble: 6 × 13</caption>
<thead>
	<tr><th scope=col>pH</th><th scope=col>MOI</th><th scope=col>TN</th><th scope=col>TP</th><th scope=col>AP</th><th scope=col>NH4</th><th scope=col>NO3</th><th scope=col>SOC</th><th scope=col>MAT</th><th scope=col>MAP</th><th scope=col>PBM</th><th scope=col>PCV</th><th scope=col>PSR</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>-1.6396547</td><td>-0.1610384</td><td>-0.1162406</td><td>-0.44892589</td><td> 0.17966965</td><td> 0.4471754</td><td>-0.5148515</td><td>-0.292278803</td><td> 0.8420221</td><td> 0.02487222</td><td> 0.17638650</td><td> 0.426595458</td><td> 0.2275781</td></tr>
	<tr><td> 0.9429126</td><td>-0.5549069</td><td>-0.7919730</td><td> 0.02952209</td><td>-0.78701735</td><td>-0.3426843</td><td>-0.4197756</td><td>-0.697789311</td><td>-1.5519670</td><td> 0.27013035</td><td>-0.02465146</td><td> 0.008333099</td><td>-0.8352947</td></tr>
	<tr><td>-1.3387571</td><td>-0.2762522</td><td> 0.1998817</td><td>-0.14900328</td><td>-0.10273314</td><td> 0.7179844</td><td>-0.4922144</td><td> 0.461059956</td><td>-0.8480169</td><td> 1.21243154</td><td> 0.73829039</td><td>-0.793336422</td><td> 0.8349340</td></tr>
	<tr><td>-1.2035947</td><td>-0.1658411</td><td>-0.2600847</td><td>-0.45606691</td><td> 0.09277663</td><td>-0.1328073</td><td>-0.1677489</td><td>-0.007572046</td><td> 0.7833595</td><td>-0.87351309</td><td>-0.80968336</td><td>-0.514494850</td><td>-1.2908116</td></tr>
	<tr><td>-1.1762407</td><td> 1.0043821</td><td> 0.2918750</td><td>-0.47748995</td><td> 2.24338416</td><td> 0.7924569</td><td>-0.5827629</td><td> 0.035809220</td><td>-0.2893264</td><td>-0.22471654</td><td> 0.12645186</td><td> 0.182609082</td><td> 0.9867730</td></tr>
	<tr><td> 0.4263991</td><td> 0.5467350</td><td>-0.3570957</td><td>-0.12043922</td><td> 0.54896611</td><td>-0.3652517</td><td>-0.5540892</td><td>-0.360483891</td><td>-0.8480169</td><td> 1.21243154</td><td> 2.11789718</td><td>-0.026522097</td><td> 0.6830950</td></tr>
</tbody>
</table>
<table>
<caption>A tibble: 6 × 5</caption>
<thead>
	<tr><th scope=col>pp</th><th scope=col>ba</th><th scope=col>fu</th><th scope=col>pr</th><th scope=col>om</th></tr>
	<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><td>  2.550000</td><td>28.010000</td><td> 2.550000</td><td>2.550000</td><td>15.280000</td></tr>
	<tr><td>  1.311046</td><td> 7.316707</td><td> 1.337884</td><td>1.337884</td><td> 5.953381</td></tr>
	<tr><td>  8.250000</td><td>13.255000</td><td> 3.355000</td><td>0.000000</td><td>14.850000</td></tr>
	<tr><td>102.490000</td><td>18.516667</td><td>20.946667</td><td>0.000000</td><td>23.500000</td></tr>
	<tr><td> 21.350000</td><td>52.720000</td><td>14.846667</td><td>0.000000</td><td> 5.623333</td></tr>
	<tr><td>  2.490000</td><td> 4.965000</td><td> 1.245000</td><td>0.000000</td><td> 2.475000</td></tr>
</tbody>
</table>
<pre><code class="language-R">library('vegan')

# Performing RDA and viewing the results
res_rda &lt;- rda(df_com, df_env)
res_rda

# Performing ENVFIT
res_envfit &lt;- envfit(df_com, df_env)

# However, the result of ENVFIT is not in the data.frame format, we should extract useful information from it.
res_envfit
</code></pre>
<pre><code>Call: rda(X = df_com, Y = df_env)

                Inertia Proportion Rank
Total         2031.7341     1.0000     
Constrained    895.7317     0.4409    5
Unconstrained 1136.0024     0.5591    5
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2  RDA3  RDA4  RDA5 
754.0  88.3  45.4   6.2   1.8 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5 
747.4 235.0  96.3  44.7  12.6 





***VECTORS

          pp       ba     r2 Pr(&gt;r)    
pH  -0.94935  0.31421 0.0809  0.003 ** 
MOI  0.79143  0.61126 0.2636  0.001 ***
TN   0.91399  0.40574 0.0887  0.004 ** 
TP   0.86358 -0.50421 0.0002  0.984    
AP   0.99269  0.12071 0.0575  0.026 *  
NH4  0.94495 -0.32722 0.0462  0.033 *  
NO3  0.52786  0.84933 0.1430  0.002 ** 
SOC  0.81629  0.57764 0.1440  0.001 ***
MAT  0.10537 -0.99443 0.0118  0.405    
MAP -0.18615 -0.98252 0.0073  0.579    
PBM  0.75360  0.65733 0.0096  0.508    
PCV  0.57508  0.81810 0.0380  0.072 .  
PSR  0.88521 -0.46519 0.0062  0.639    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Permutation: free
Number of permutations: 999
</code></pre>
<p>正如我们在注释提到的，ENVFIT 并不是以 data.frame 的形式提供数据，因此我们还需要通过一些提取的操作才能获得与结果相同的表格。由于这类操作经常使用，因此我们将其包装成 function 并命名为 envfit_to_df</p>
<pre><code class="language-R"># Here, env_obj indicates the result of envfit. In this case, it's the res_envfit.
# r2_dig is the significant figure of R2
# p_dig is the significant figure of p value
envfit_to_df &lt;- function(env_obj,
                         r2_dig = 6,
                         p_dig = 3) {
  r2_fmt &lt;- as.character(paste('%.', r2_dig, 'f', sep = ''))
  p_fmt &lt;- as.character(paste('%.', p_dig, 'f', sep = ''))
  tibble(
      # the name of explainary variables
    factor = names(env_obj$vectors$r),
      # list or vector of R2
    r2 = env_obj$vectors$r,
      # list or vector of p values
    pvals = env_obj$vectors$pvals
  ) %&gt;%
    # generate significant levels by p values
    mutate(sig = case_when(
      pvals &lt;= 0.001 ~ '***',
      pvals &lt;= 0.01 ~ '**',
      pvals &lt;= 0.05 ~ '*',
      TRUE ~ ' '
    )) %&gt;%
    # format the significant figure by format definition before.
    mutate(pvals = sprintf('%.3f', pvals),
           r2 = sprintf(r2_fmt, r2))
}
# Convert result of ENVFIT to data.frame, in fact, it's a tibble.
df_env &lt;- envfit_to_df(res_envfit, r2_dig = 3)
df_env
</code></pre>
<table>
<caption>A tibble: 13 × 4</caption>
<thead>
	<tr><th scope=col>factor</th><th scope=col>r2</th><th scope=col>pvals</th><th scope=col>sig</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>pH </td><td>0.081</td><td>0.003</td><td>** </td></tr>
	<tr><td>MOI</td><td>0.264</td><td>0.001</td><td>***</td></tr>
	<tr><td>TN </td><td>0.089</td><td>0.004</td><td>** </td></tr>
	<tr><td>TP </td><td>0.000</td><td>0.984</td><td>   </td></tr>
	<tr><td>AP </td><td>0.058</td><td>0.026</td><td>*  </td></tr>
	<tr><td>NH4</td><td>0.046</td><td>0.033</td><td>*  </td></tr>
	<tr><td>NO3</td><td>0.143</td><td>0.002</td><td>** </td></tr>
	<tr><td>SOC</td><td>0.144</td><td>0.001</td><td>***</td></tr>
	<tr><td>MAT</td><td>0.012</td><td>0.405</td><td>   </td></tr>
	<tr><td>MAP</td><td>0.007</td><td>0.579</td><td>   </td></tr>
	<tr><td>PBM</td><td>0.010</td><td>0.508</td><td>   </td></tr>
	<tr><td>PCV</td><td>0.038</td><td>0.072</td><td>   </td></tr>
	<tr><td>PSR</td><td>0.006</td><td>0.639</td><td>   </td></tr>
</tbody>
</table>
<p>截止到目前，我们已经准备完成了 RDA 分析以及 ENVFIT 分析，并将数据转换成为了满足可视化需求的格式。</p>
<p>接下来就是需要进行可视化作业。此例，笔者仅需要显示环境变量与物种信息，不需要显示样地信息，因此绘制中仅保留了所需的信息，如需显示样地信息，请按实际需求更改。</p>
<p>绘制 RDA 图形是常用的操作，因此同样将它包装成为 function 并命名为 <code>ggRDA</code></p>
<pre><code class="language-R"># Here, rda_obj means the object which is from vegan::rda
# sp_size means the text size of species
# arrow_txt_size means the environmet variable names at the end of the arrow
# Because not every RDA plot needs indicate significant correlations, the envfit_df is optional here.
ggRDA &lt;- function(rda_obj,sp_size = 4,arrow_txt_size = 4, envfit_df) {
    # ggplot doesn't support rda object directly, we use ggvegan::fortify function to convert the rda to data.frame
    fmod &lt;- fortify(rda_obj)
    # to get the arrow of biplot, we plot rda by vegan::plot.rda function firstly. 
    # The arrow attributes contain in the attributes(plot_obejct$biplot)$arrow.mul
    basplot &lt;- plot(rda_obj)
    mult &lt;- attributes(basplot$biplot)$arrow.mul
  
    # To check if envfit_df exists or not
    # If envfit_df exists, join the fortified rda_obj and envfit to mark which variable is significant.
    if(missingArg(envfit_df)){
        bplt_df &lt;- filter(fmod, Score == &quot;biplot&quot;) %&gt;%
        # If there is no requirement to mark significant variable
        # use the sytle of sinificant (black bolder solid arrow)
        # to paint the arrow
        mutate(bold = 'sig')
    } else {
        bplt_df &lt;- filter(fmod, Score == &quot;biplot&quot;) %&gt;%
        left_join(envfit_df, by = c('Label' = 'factor')) %&gt;%
        # To mark the significant variables as sig, not significant variables as ns
        # these information are stored in bold column
        mutate(bold = ifelse(str_detect(sig, fixed('*')), 'sig', 'ns'))
    }
  ggplot(fmod, aes(x = RDA1, y = RDA2)) +
    coord_fixed() +
    geom_segment(data = bplt_df,
                 # mult and RDA1/RDA2 are from fortified RDA data.frame
                 # they contain the direction and effects of every variabl
                 # their products are the direction and length of arrows
                 aes(x = 0, xend = mult * RDA1,
                     y = 0, yend = mult * RDA2,
                     # Use different arrow size to indicate the significant level
                     size = bold,
                     # Use different arrow color to indicate the significant level
                     color = bold,
                     # Use different arrow linetype to indicate the significant level
                     linetype = bold),
                 #############################
                 # Q:Why use three different attibution to control the significant levels?
                     # It is redundancy, isn't it?
                 # A: In fact, it's not easy to recongize the significant level by one kind attribution
                     # Becasue it is not delicate to indicate it with supper bold and supper thin arrow,
                     # by the same logic, high contrast colors are not delicate neither.
                     # As for the line type, some arrow are really short, it's not easy to recognize
                     # weather it is solid or dashed line at all.
                     # To sum up, we use three different attributions
                     # to indicate the same difference to avoid any misleading.
                 #############################
                 # to control the size of the header of arrow
                 arrow = arrow(length = unit(0.25, &quot;cm&quot;)), 
                ) +
    # Add the text of variable name at the end of arrow
    geom_text(data =  subset(fmod, Score == &quot;biplot&quot;),
              aes(x = (mult + mult/10) * RDA1,
                   #we add 10% to the text to push it slightly out from arrows
                  y = (mult + mult/10) * RDA2,
                  label = Label),
              size = arrow_txt_size,
               #otherwise you could use hjust and vjust. I prefer this option
              hjust = 0.5) +
    # Add the text of species
    geom_text(
      data = subset(fmod, Score == &quot;species&quot;),
      aes(colour = &quot;species&quot;,label = Label),
      size = sp_size
    )
}
</code></pre>
<p>虽然使用 ggRDA 可以直接绘制图形，但通常为了美观，对于特定的参数还需要进一步的调整。</p>
<p>注意，由于在 ggRDA 中使用了 vegan::plot.rda 绘制图像，所以在下面的调用 ggRDA 会首先绘制一次简易的 rda 图像之后，再显示出 ggplot2 绘制的图形，不影响后续的输出保存。</p>
<pre><code class="language-R">library(ggvegan)
# Get the amount of explanation by each axis
# gernerally, we choose the first two axes.
exp_by_x &lt;- (as.list(res_rda$CCA$eig)$RDA1)/(res_rda$tot.chi) * 100
exp_by_y &lt;- (as.list(res_rda$CCA$eig)$RDA2)/(res_rda$tot.chi) * 100
    
ggRDA(res_rda, envfit_df = df_env, sp_size = 5) +
    # Generally theme_classic is a good choice to paint a figure
    theme_classic() +
    # In general, we don't need to show the legend in RDA figure
    theme(legend.position = &quot;none&quot;) +
    xlab(paste('RDA1 (', round(exp_by_x, 2), '%)', sep = '')) +
    ylab(paste('RDA2 (', round(exp_by_y, 2), '%)', sep = '')) +
    # scale_XXXXX_manual series provide the ability
    # to define the style of legend by variable value
    scale_size_manual(values = c('ns' = .6,
                                 'sig' = .8)) +
    # Q: What's species here? I don't remember their is a significant level which is called 'species'
    # A: Indeed, their is no significant 'species'. However, 
        # the species name in RDA which is generated from geom_text contains colour attribution.
    scale_colour_manual(values = c(
        'ns' = '#606060',
        'sig' = 'black',
        'species' = 'red'
      )) +
    scale_linetype_manual(values = c('ns' = 8, 'sig' = 1))
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/output_8_1.png" alt="" loading="lazy"></figure>
<p>最后很简单了，使用 ggsave 将 RDA 图保存成你需要的格式就可以。</p>
<p>Tips: 自然科学期刊的投稿系统通常支持上传 PDF 格式的图片，根据实际情况也推荐使用 ggsave 输出 PDF 格式。</p>
<p>此时无需设置 DPI 也无法设置 DPI，这是因为，ggsave 保存的 PDF 文件会优先将图片输出为矢量图，简而言之就是图片无论如何放大，都不会变得模糊，而期刊的排版系统也能很好的处理这种矢量图。</p>
<p>只要给 PDF 图片设定一个合适的宽高，就无需担心图片会变得不清晰等奇奇怪怪看似玄学的问题了。</p>
<p>不过如果有修改字体的需求，也许 PDF 会报错，类似于字体无法嵌入，此时输出的 PDF 文件所有的文字都会消失。关于解决这个问题，后续也许会进行进一步的讨论。</p>
<pre><code class="language-R">ggsave('RDA.pdf',width = 7)
</code></pre>
<pre><code>Saving 7 x 7 in image
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[设备管理器中的 MVSI Card Reader USB Device driver 是什么？如何移除其占用盘符？]]></title>
        <id>https://womeimingzi11.github.io/post/she-bei-guan-li-qi-zhong-de-mvsi-card-reader-usb-device-driver-shi-shi-me-ru-he-yi-chu-qi-zhan-yong-pan-fu/</id>
        <link href="https://womeimingzi11.github.io/post/she-bei-guan-li-qi-zhong-de-mvsi-card-reader-usb-device-driver-shi-shi-me-ru-he-yi-chu-qi-zhan-yong-pan-fu/">
        </link>
        <updated>2020-03-23T07:58:19.000Z</updated>
        <content type="html"><![CDATA[<p>最近微电脑添置了一块机械硬盘，毕竟在 SSD 价格飞涨的这个时期里，原本计划一步到位 1T NVME 的计划被腰斩至 512 GB。虽然日常的使用中不会遇到任何问题，但是想要保存一些资料依然有捉襟见肘的局促感。</p>
<p>然而当硬盘安装停当，需要设定盘符之时，却发现盘符编号已经被占用了。原本电脑中的 NVME 硬盘仅一个分区 C 盘，按照预想机械硬盘应该紧接着设定为 D 盘。然而设定菜单中新盘符却只能 E 盘起跳，这说明有一个设备占用了 D 盘。</p>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_21-27-53.png" alt="" loading="lazy"></figure>
<p>然而实际情况是电脑除了一块 NVME 硬盘并没有其他的存储设备。查看设备管理器的磁盘把驱动器组。发现除了 <strong>NVME 硬盘</strong>（HS-SSD-C2000pro 512G 具体产品是海康 C2000 Pro）、<strong>机械硬盘</strong> （HGST XXXX 日立企业硬盘）、<strong>Hyper-V 虚拟硬盘</strong>（Microsoft 虚拟磁盘，这个与本次的讨论无关）之外，还有一个名为 <strong>MVSI Card Reader USB Device driver</strong> 的设备。</p>
<figure data-type="image" tabindex="2"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_21-29-47.png" alt="Snipaste_2020-03-22_21-29-47" loading="lazy"></figure>
<p>按照设备的名字，大概是一款 USB 读卡器。然而我的机箱本身(TT 启航者 F1 静音版)并没有读卡器，况且我也没有接入其他的 USB 读卡器，莫非是鼠标键盘又抽风了——毕竟曾经遇到过 USB 键鼠插入电脑后无法引导设备的奇葩故障。</p>
<p>不过在尝试插拔键鼠之前，还是决定先把这个 MVSI Card Reader USB Device driver 搜一搜看看是个什么东西。不幸的是搜到的有效结果并不是非常丰富。</p>
<figure data-type="image" tabindex="3"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_21-31-24.png" alt="Snipaste_2020-03-22_21-31-24" loading="lazy"></figure>
<p>虽然第一个条目似乎是设备的驱动下载，但是点进去就有一股浓浓的不能解决问题的感觉。简介基本上是模式化的内容，并没有设备本身相关的内容。截图也是与文无关的样子，虽然提供了驱动程序下载的链接，但是我并不需要下载驱动呀，Windows 已经帮我装好驱动了，故此先将设备放在一边。</p>
<figure data-type="image" tabindex="4"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_22-14-33.png" alt="Snipaste_2020-03-22_22-14-33" loading="lazy"></figure>
<p>第二个搜索条目……日语的？先不看了……</p>
<p>第三个搜索条目，欸，JBL Pebbles？虽然页面内并没有出现任何 MVSI 字样，但是巧合的是，我的电脑上也接入了一台 JBL Pebbles USB 音箱。</p>
<p>此时再扫一眼第二条搜索条目，似乎说到了 uaudio0 字样？虽然对于设备 unix 下的访问名称不熟悉——其实插到我的 MBP 下就能查看了，但是谁让我懒呢——不过 audio 大概是音频设备的相关的吧。耐着性子看作者的 Tweet，似乎在说自己的 JBL Pebbles 插入电脑后也会有一个 Removable SCSI 设备，名字也是  MVSI Card Reader USB Device driver。</p>
<figure data-type="image" tabindex="5"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_21-30-53.png" alt="Snipaste_2020-03-22_21-30-53" loading="lazy"></figure>
<p>考虑到巧合不会同时出现两次（无来源，我现编的），果断拔下 JBL Pebbles，设备管理器中的  MVSI Card Reader USB Device driver 设备消失，在磁盘管理器中也能将机械硬盘设置成为 D 盘了，done！</p>
<figure data-type="image" tabindex="6"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_22-17-07.png" alt="Snipaste_2020-03-22_22-17-07" loading="lazy"></figure>
<p>事后回想起来，猜测是因为采用 USB 连接电脑的 JBL Pebbles 音箱会采用 USB DAC 的方式来实现工作。但巧合的是这台 USB DAC 还预留了读卡器的接口，虽然 JBL Pebbles 并没有读卡的功能。</p>
<p>不过虽然盘符的问题已经解决了，但是其实还有一个小瑕疵，那就是 Windows Traybar 里面会一直有一个快速移除设备的图标，并且总有一个省略号设备，且无法点击。</p>
<figure data-type="image" tabindex="7"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_22-09-17.png" alt="Snipaste_2020-03-22_22-09-17" loading="lazy"></figure>
<p>解决方法也很简单，只要从设备管理器中选中 MVSI Card Reader USB Device driver 然后将其禁用即可。目前没有发现副作用。</p>
<figure data-type="image" tabindex="8"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/Snipaste_2020-03-22_22-17-07.png" alt="Snipaste_2020-03-22_22-17-07" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[R for Everything: 使用 ggplot2 可视化新型冠状病毒肺炎在中国的确诊分布]]></title>
        <id>https://womeimingzi11.github.io/post/map_visualization_covid19_by_ggplot2/</id>
        <link href="https://womeimingzi11.github.io/post/map_visualization_covid19_by_ggplot2/">
        </link>
        <updated>2020-03-08T13:36:10.000Z</updated>
        <content type="html"><![CDATA[<p>这篇文章受到 Anisa Dhana 的博文 <a href="https://datascienceplus.com/map-visualization-of-covid19-across-world/">‘Map Visualization of COVID-19 Across<br>
the World with R’</a> 启发，尝试使用 R 语言绘制新冠肺炎 COVID-19 在国内的确诊、治愈和死亡地图。</p>
<h2 id="数据准备">数据准备</h2>
<h3 id="covid-19-data">COVID-19 Data</h3>
<p>绘制地图的第一步是收集数据。</p>
<p>首先，我们感谢约翰霍普金斯 CSSE <a href="https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases">Johns Hopkins CSSE</a>, 他们将全球的疫情数据按照日期制作成了单一的 CSV 文件方便进行各类分析和统计。我们可以从 <a href="https://github.com/CSSEGISandData/COVID-19">Github</a> 页面获得需要的数据。</p>
<p>P.S. 非常建议大家直接使用 read_csv 从源 URL 直接获取数据，这样便于日后维护更新。但是因为个人原因，案例中使用下载至本地的 csv 作为数据源。此操作不影响后续的任务。</p>
<pre><code>library(tidyverse)

# Read the daily CSV file from Jons Hopkins CSSE I highly recommand to read_csv
# from url directly. But for my personal reason I have to download csv and read
# it from local path.

# df_origin &lt;-
# read_csv('http://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/03-07-2020.csv')
df_origin &lt;- read_csv(&quot;03-07-2020.csv&quot;)

# Replace the slash into underline to make it easyily to filter
names(df_origin) &lt;- names(df_origin) %&gt;% str_replace_all(&quot;/&quot;, &quot;_&quot;)

# Keep the data of mainland and three regions of China
df_cov_china &lt;- filter(df_origin, Country_Region %in% c(&quot;Mainland China&quot;, &quot;Taiwan&quot;, 
    &quot;Hong Kong&quot;, &quot;Macao&quot;))

head(df_cov_china)

## # A tibble: 6 x 8
##   Province_State Country_Region `Last Update`       Confirmed Deaths Recovered
##   &lt;chr&gt;          &lt;chr&gt;          &lt;dttm&gt;                  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;
## 1 Hubei          Mainland China 2020-03-07 11:13:04     67666   2959     43500
## 2 Guangdong      Mainland China 2020-03-07 10:43:02      1352      7      1237
## 3 Henan          Mainland China 2020-03-07 11:23:10      1272     22      1244
## 4 Zhejiang       Mainland China 2020-03-07 09:03:05      1215      1      1154
## 5 Hunan          Mainland China 2020-03-07 09:03:05      1018      4       960
## 6 Anhui          Mainland China 2020-03-06 03:23:06       990      6       979
## # ... with 2 more variables: Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;
</code></pre>
<h3 id="map-data">Map Data</h3>
<p>我们采用了包含两岸三地、藏南、阿克赛钦以及南海九段线的完整中华人民共和国的行政区划地图。这些数据将会托管在 <a href="https://raw.githubusercontent.com/womeimingzi11/womeimingzi11.github.io/master/mapData.zip">Github</a> 上进行分享，大家可以按需取用。</p>
<pre><code>library(rgdal)

# Read China Admistrative Area data from Shapefile.  To avoid the compatibility
# issue across systems, including Unix-like system and Windows. I highly
# recommand to use the file.path function to create the file paths.
map_cn_area &lt;- file.path(&quot;mapData&quot;, &quot;China_adm_area.shp&quot;) %&gt;% readOGR()

## OGR data source with driver: ESRI Shapefile 
## Source: &quot;C:\Users\chenh\OneDrive\Develop Learn\R\Map Visualization of COVID-19 Across China with R\mapData\China_adm_area.shp&quot;, layer: &quot;China_adm_area&quot;
## with 34 features
## It has 10 fields

# Conver the SpatialPolygonsDataFrame to DataFrame which can be held by ggplot2
df_cn_area &lt;- fortify(map_cn_area)

# Read the names of Province or region, and convert the name from PinYin to
# English.  This can match the Province name between WHO data and map data.
ls_province_name &lt;- map_cn_area@data$ID %&gt;% str_replace(&quot;Xianggang&quot;, &quot;Hong Kong&quot;) %&gt;% 
    str_replace(&quot;Aomen&quot;, &quot;Macao&quot;)

# The id is the unique serial to recongize the different province from map data.
ls_id &lt;- unique(df_cn_area$id)

# The orders of id and province name is all the same, the bind operation will
# combine the province name and id in different data.frame Use Proveince_State to
# Join data from WHO data and map data
df_final &lt;- df_cn_area %&gt;% left_join(bind_cols(Province_State = ls_province_name, 
    id = ls_id)) %&gt;% left_join(df_cov_china, by = &quot;Province_State&quot;)

# Read the boundary of provinces and regions from shapefile, it will be a
# SpatialLinesDataFrame
map_cn_bord &lt;- file.path(&quot;mapData&quot;, &quot;China_adm_bord.shp&quot;) %&gt;% readOGR()

## OGR data source with driver: ESRI Shapefile 
## Source: &quot;C:\Users\chenh\OneDrive\Develop Learn\R\Map Visualization of COVID-19 Across China with R\mapData\China_adm_bord.shp&quot;, layer: &quot;China_adm_bord&quot;
## with 1785 features
## It has 8 fields
## Integer64 fields read as strings:  FNODE_ TNODE_ LPOLY_ RPOLY_ BOU2_4M_ BOU2_4M_ID

df_cn_bord &lt;- fortify(map_cn_bord)
</code></pre>
<h2 id="数据可视化">数据可视化</h2>
<p>当所有的数据准备完成，便可以使用 ggplot2 进行可视化操作。使用 geom_polygon 来绘制不同行政区，因为数据类型是 <code>SpatialPolygonsDataFrame</code> 而使用 geom_path 来绘制行政区的边界，因为数据类型是<code>SpatialLinesDataFrame</code>。</p>
<p>我们需要额外考虑的问题是，因为武汉的患者数量数倍于国内其他区域范围内的患者数量，故此为了能够较为有层次的显示病患数量，我们将数据进行<em>平方根开方</em>处理。</p>
<pre><code>library(ggplot2)
# The group is unique serial of each province and region, in this case, it is
# similar with id.  Use geom_polygon to plot the area part, and use geom_path to
# plot the boundary part.  To make the data can be comparable, the patients in
# Hubei Province are multiple times than those in other provinces and regions,
# the data is root-square transformed.
ggplot() + geom_polygon(aes(x = long, y = lat, group = group, fill = sqrt(Confirmed)), 
    data = df_final) + geom_path(aes(x = long, y = lat, group = group), color = &quot;black&quot;, 
    data = df_cn_bord) + labs(caption = &quot;Data Repository provided by Johns Hopkins CSSE. Visualization by Han Chen&quot;) + 
    guides(fill = guide_legend(title = &quot;确诊人数\n（开平方处理）&quot;)) + theme(text = element_text(color = &quot;#22211d&quot;), 
    plot.background = element_rect(fill = &quot;#ffffff&quot;, color = NA), panel.background = element_rect(fill = &quot;#ffffff&quot;, 
        color = NA), legend.background = element_rect(fill = &quot;#ffffff&quot;, color = NA))
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/mapVisualization_confirmed-1.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[R for Everything: 使用 ggplot2 绘制 RasterLayer 地图]]></title>
        <id>https://womeimingzi11.github.io/post/using-r-for-everything-shi-yong-ggplot2-hui-zhi-rasterlayer-di-tu/</id>
        <link href="https://womeimingzi11.github.io/post/using-r-for-everything-shi-yong-ggplot2-hui-zhi-rasterlayer-di-tu/">
        </link>
        <updated>2020-02-19T12:32:06.000Z</updated>
        <content type="html"><![CDATA[<p>author: Han (<a href="mailto:chenhan28@gmail.com" class="email">chenhan28@gmail.com</a>)<br>
在最近刚刚完成的一篇 SCI 文章中，为了描述实验的采样范围，通过 ggplot2 包 (Wickham et al. 2019) 将一组 RasterLayer 绘制成为了青藏高原的地形图。考虑到使用 R 绘制地图的中文内容较少，我们进行一次回顾。</p>
<p><strong>PS 因为不是地理方面的文章/专业，所以在专业性方面有欠缺，但对于自然科学类文章中进行展示基本上是足够了。</strong></p>
<h2 id="为什么用-ggplot2-画地图">为什么用 ggplot2 画地图？</h2>
<p><strong>因为我能！（摊手</strong></p>
<p>实际上原因如下：</p>
<ol>
<li>
<p>ggplot2 是非常强大的绘图工具，配合上 ggplot2 的衍生包，这套工具链基本能满足生态学领域几乎所有的绘图需求。</p>
</li>
<li>
<p>如果对 Photoshop 这类图像处理软件熟悉，就会发现使用 ggplot2 画图，逻辑上和 PS 是非常相似的，便于快速上手和修改生成的图像——天知道把英文图改成中文图有多烦人</p>
</li>
<li>
<p>此外相比于 ArcGIS 这类软件 R 这类跨平台软件几乎可以在任何环境下完成绘图任务，甚至可以在家中的机顶盒安装 R，只不过慢到天长地久而已。</p>
</li>
<li>
<p>免费免费免费</p>
</li>
</ol>
<p>在开始之前，先来看看最终的展示效果。<strong>为了避免文章版权和数据共享问题，地图上样点均去除，仅供参考。</strong></p>
<figure data-type="image" tabindex="1"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/gmap.jpg" alt="Demo without points" loading="lazy"></figure>
<h2 id="什么是-rasterlayer">什么是 RasterLayer？</h2>
<p>关于 RasterLayer 的定义，在 <a href="https://rspatial.org/raster/spatial/4-rasterdata.html">Spatial Data Science (Feed the Future n.d.)</a> 中有很好的解释。</p>
<blockquote>
<p>A RasterLayer object represents single-layer (variable) raster data. A RasterLayer object always stores a number of fundamental parameters that describe it. These include the number of columns and rows, the spatial extent, and the Coordinate Reference System. In addition, a RasterLayer can store information about the file in which the raster cell values are stored (if there is such a file). A RasterLayer can also hold the raster cell values in memory.</p>
</blockquote>
<p>在 R 中提及的 RasterLayer 通常指的是由 sp 包 (Pebesma et al. 2019) 提供的 RasterLayer 类，每一个 RasterLayer 代表一层 raster 栅格数据，其中记录了 raster 数据的基础信息，例如行、列、空间范围、参考系。而对 RasterLayer 进行操作最常用的工具是 raster 包 (Hijmans et al. 2020)。</p>
<h2 id="数据准备">数据准备</h2>
<p>所需加载包： 1. <code>elevatr</code>(Hollister and Shah 2018)；2. <code>raster</code>； 3. <code>tidyverse</code> (Wickham and RStudio 2019)</p>
<p>具体到这一次的地图绘制中，我们需要<strong>两个</strong> RasterLayer —— 1. 作为背景层的 <code>bg_rst</code>，以及 2.用作展示地形的 <code>tp_rst</code>。那么如何获得这两个 RasterLayer 呢？<code>elevatr</code> 包提供了专门用于获取高程栅格数据的方法 <code>get_elev_raster</code>.</p>
<p>不过在获取高程数据之前，需要首先指定地图绘制矩形边界。之后方可使用 <code>get_elev_raster</code> 来获取边界范围内的高程数据，使用 <code>z</code> 参数 (zoom) 确定缩放程度。因为通过 <code>get_elev_raster</code> 获取高程 raster 的方法是获取服务器与自定义边界的最小公倍数（不准确的说法），所以需要对获取的原始 RasterLayer 再次剪切，以便得到地图绘制矩形边界内的数据。</p>
<pre><code class="language-r">library(elevatr)  # Get rasterlay from AWS by `get_elev_raster` fucntion
library(raster)  # Manipulate RasterLayer object
library(tidyverse)  # Tidy is everything.

# Set the extent we want to plot
ext_sample &lt;- extent(70, 105, 25, 45)

# Preparing for getting the elevation raster data, make a blank RasterLayer,
# becasue the first parameter of get_elev_raster is a target Rasterlayer.
bg_init &lt;- raster(ext = ext_sample, resolution = 0.01)
# Get elevation raster with zoom 5, then only keep the extend we want to plot
# later.
bg_rst &lt;- get_elev_raster(bg_init, z = 5) %&gt;% crop(ext_sample)
</code></pre>
<p><code>bg_rst</code> 就是地图背景中灰色的辅助部分的数据就准备好了。</p>
<pre><code class="language-r"># Let's check the detail of bg_rst, the Background RasterLayer
bg_rst
</code></pre>
<pre><code>## class      : RasterLayer 
## dimensions : 1075, 1591, 1710325  (nrow, ncol, ncell)
## resolution : 0.022, 0.0186  (x, y)
## extent     : 70.008, 105.01, 25.0029, 44.9979  (xmin, xmax, ymin, ymax)
## crs        : +proj=longlat +datum=WGS84 +no_defs 
## source     : memory
## names      : layer 
## values     : -156.1392, 8006.811  (min, max)
</code></pre>
<p>随后我们需要下载青藏高原的多边形文件，这里我们选择张镱锂 (2002) 等人在《论青藏高原范围与面积》一文提供的青藏高原范围与界线地理信息系统数据。从<a href="http://www.geodoi.ac.cn/WebCn/doi.aspx?Id=135">《全球变化科学研究数据出版系统》</a>下载即可。这里我选择了 <code>DBATP.zip</code> 下载，对应的文件格式为 Shaplefile，使用 rgdal 包 (Bivand et al. 2019) 提供的 <code>readOGR</code> 方法读取其中的 <code>DBATP_Polygon.shp</code>，保存的数据类型为<code>tp_ext</code>（类型为 SpatialPolygonsDataFrame）。之后将<code>bg_rst</code> 数据按照 <code>tp_ext</code> 形状进行处理，获得符合青藏高原范围的 RasterLayer <code>tp_rst</code>。</p>
<pre><code class="language-r">library(rgdal)
# Read the SpatialPolygon File from DBATP_Polygon.shp
tp_ext &lt;- readOGR(&quot;DBATP/DBATP_Polygon.shp&quot;)
</code></pre>
<pre><code>## OGR data source with driver: ESRI Shapefile 
## Source: &quot;/Users/chenhan/OneDrive/Develop Learn/R/Blog_Archive/plotMapByGGplot/DBATP/DBATP_Polygon.shp&quot;, layer: &quot;DBATP_Polygon&quot;
## with 1 features
## It has 1 fields
</code></pre>
<pre><code class="language-r"># Keep the shpae of the Tibetan Plateau
tp_rst &lt;- mask(bg_rst, tp_ext)
</code></pre>
<p>为了便于定位，我们还将在图片上绘制地标名称 <code>city_ls</code>，以及采样点位置及其类型 <code>hbt_coord</code>。</p>
<p>PS.可以根据自己的实际情况确定数据的存储类型，这里因为个人项目的实际情况，数据并没有保存成为常见的 data.freame 或者 tibble 之类的类表格形式。<strong>注意绘图过程中前后对应即可</strong>。</p>
<pre><code class="language-r"># Create the list of landmarks which we want to mark
city_ls &lt;- list(x = c(91.1, 86.925278, 101.7781), y = c(29.65, 27.988056, 36.6169), 
    label = c(&quot;Lhasa&quot;, &quot;Qomolangma&quot;, &quot;Xi'Ning&quot;))
str(city_ls)
</code></pre>
<pre><code>## List of 3
##  $ x    : num [1:3] 91.1 86.9 101.8
##  $ y    : num [1:3] 29.6 28 36.6
##  $ label: chr [1:3] &quot;Lhasa&quot; &quot;Qomolangma&quot; &quot;Xi'Ning&quot;
</code></pre>
<pre><code class="language-r"># Read point with latitude and longitude. This operation is not needed for
# everyone, actually it depends on the actual data structure.
hbt_coord &lt;- read_rds(&quot;hbt_coord.rds&quot;) %&gt;% mutate(Ecosystem = ifelse(hbt == &quot;M&quot;, 
    &quot;Alpine Meadow&quot;, &quot;Alpine Steppe&quot;))
str(hbt_coord)
</code></pre>
<pre><code>## tibble [432 × 4] (S3: tbl_df/tbl/data.frame)
##  $ hbt      : chr [1:432] &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ...
##  $ lon      : num [1:432] 101 101 101 100 100 ...
##  $ lat      : num [1:432] 35.3 35 35 34.5 34.5 ...
##  $ Ecosystem: chr [1:432] &quot;Alpine Meadow&quot; &quot;Alpine Meadow&quot; &quot;Alpine Meadow&quot; &quot;Alpine Meadow&quot; ...
</code></pre>
<p><strong>注意</strong>，前面的操作中，我们裁剪 RasterLayer 用到了 crop 和 mask 两种操作，关于这两种操作的解释，我用一张图来解释：</p>
<figure data-type="image" tabindex="2"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/JPEG%E5%9B%BE%E5%83%8F-606FC26ACE54-1.jpeg" alt="Differences between crop and mask" loading="lazy"></figure>
<p>简而言之，两种操作都会得到更小的矩形图形，但是使用 mask 方法会在多边形区域外的矩形部分填充 NA 使被裁剪 RasterLayer 看起来成为多边形。</p>
<h2 id="地图绘制">地图绘制</h2>
<p>所需加载包： <code>scales</code> (Wickham, Seidel, and RStudio 2019)</p>
<p>不过在进行绘图之前，还需要对 RasterLayer 数据进行一些小的调整，以便与 <code>ggplot2</code> 的功能兼容。首先将两个 RasterLayer 转换为 data.frame 保留 xy，xy 为经纬度，应点上的值将会保留成与 RasterLayer 中 names 相同的列名，比如 <code>bg_rst</code> 转换为 data.frame 后列名就是 x, y, layer。之后我们将 layer（在此处为当前为止的海拔高度）转换数值范围，因为保留原始的数据用作后面的透明度会让整张图像灰蒙蒙。</p>
<p>不过要注意的是，正如我们前面说到的 mask 后的 RasterLayer 会将区域外的数据标记为 NA，如果直接使用 NA 绘图将会出现各种奇怪的效果，因此我们选择将 NA 数据更换为 0，将区域内的数据更换为 1，将两种值用作图像的 alpha 就会绘制出准确的青藏高原样式。</p>
<p>没看懂咋办？呆胶布！动手试试不进行 NA 转换的效果便知道了。</p>
<pre><code class="language-r"># scales package provide rescale function which can convert the range of numbers
# list to another range.
library(scales)
</code></pre>
<pre><code>## 
## Attaching package: 'scales'

## The following object is masked from 'package:purrr':
## 
##     discard

## The following object is masked from 'package:readr':
## 
##     col_factor
</code></pre>
<pre><code class="language-r"># First convert RasterLayer as Data.Frame with xy coordinate system.  Then
# rescale the elevaion to alpha, as the background part, super high alpha value
# is not a good idea, which range is the best? It depends by the actual. Save the
# alpha value with colname 'alpha'
bg_rst_df &lt;- as.data.frame(bg_rst, xy = TRUE) %&gt;% mutate(alpha = rescale(layer, to = c(0.25, 
    0.75)))

# NA will be generated by the mask function, if use NA and evelation as the alpha
# of Topographic figure, it will be dizzy, and for color Topographic figure
# please don't use the greyscale and color for the evelation simultaneously. Just
# use alpha to control the shape of regional shpae.
tp_rst_df &lt;- as.data.frame(tp_rst, xy = TRUE) %&gt;% mutate(alpha = ifelse(is.na(layer), 
    0, 1))
</code></pre>
<p>当数据准备完毕，我们就开始图形的绘制。首先进行地形图叠加到背景地形的绘制。因为命令较多，并且均以注释的形式标注到代码中，故此不再提前讲解。</p>
<pre><code class="language-r"># sacle_parm as a parameter controls the sclae of the hole figure, it will be
# used to control the size of text, point, legend, etc. to fit the size of
# figure.
scale_parm &lt;- 2
# Init ggplot
gmap &lt;- ggplot() + # plot the backgroun layer, set the alpha without color will make a grey
# background
geom_raster(data = bg_rst_df, aes(x = x, y = y, alpha = alpha)) + # plot the topographic layer, set alpha to keep the shape of Tibetan Plateau
# (TP). Color indicates the elevation.
geom_raster(data = tp_rst_df, aes(x = x, y = y, fill = layer, alpha = alpha)) + # terrain.colors is an built-in function to generate a list of color palettes.
# set the legend title of evelation by name parameter.
scale_fill_gradientn(colours = terrain.colors(100), name = &quot;Elevation (m)&quot;) + # As we said before, the alphas is used to determine the shpae of TP, we don't
# need to show them as legends.
scale_alpha(guide = &quot;none&quot;) + # Project this figure as a map but not a normal figure
coord_quickmap() + # Set preset theme makes things easire
theme_minimal() + # Set the limititions of axes. `expand` parameter will remove the gaps between
# the rectangle map and axes.  If you are not sure what's this mean, remove the
# parameter by yourself and you will find it out.
scale_x_continuous(limits = c(70, 105), expand = c(0, 0)) + scale_y_continuous(limits = c(25, 
    45), expand = c(0, 0)) + # Set the titles of axis
labs(x = &quot;Longtitude (E)&quot;, y = &quot;Laitude (N)&quot;) + # remove the background color and background grid, you know the classical
# ggplot's grid, don't you?
theme(panel.grid = element_blank(), panel.background = element_blank()) + # Set the size of axis and legend
theme(axis.title = element_text(size = 7 * scale_parm), axis.text = element_text(size = 6 * 
    scale_parm)) + theme(legend.key.width = unit(0.2 * scale_parm, &quot;cm&quot;), legend.key.height = unit(0.5 * 
    scale_parm, &quot;cm&quot;), legend.text = element_text(size = 5 * scale_parm), legend.title = element_text(size = 6 * 
    scale_parm))

# Preview will slow down the process of operations, I highly recommand do not
# preveiw the ggplot and save it as a file directly.
</code></pre>
<p>上述操作完毕，如果没有意外，就可以获得一张效果尚可的底图了。但是，个人强烈建议不进行预览图像，直接进行后续的操作，因为绘制当前精度的底图需要花费较长的时间。或者可以使用 <code>ggsave</code> 方法输出为文件进行预览，这样如果效果满意，可以直接用作成品，避免预览后再次绘制效率较低。</p>
<p>随后我们再将地标为止添加到底图上。</p>
<pre><code class="language-r">gmap &lt;- gmap +
  # Add the city_ls to the main plot as landmarks.
  geom_text(
    mapping = aes(x = x, y = y, label = label),
    # geom_text don't support the structure we used. 
    # convert the list into data.frame, every element is used as column here.
    data = bind_cols(city_ls),
    size = 2 * scale_parm
  )
</code></pre>
<p>最后将采样点添加到底图上。注意 ⚠️ 由于版权和数据分享的原因，我将采样点的坐标设置为 0，0，故此图片上不会显示任何采样点，请根据实际情况设置！</p>
<pre><code class="language-r">gmap &lt;- gmap +
  # Add the sample sites to the main plot as points.
  # Due to the copyright of my scholar article and data share policy, I won't point my sample sites to the picuture, the coordinations of point is 0,0 here.
  geom_point(
    mapping = aes(
      # x = lon,
      x = 0,
      y = 0,
      # y = lat,
      col = Ecosystem,
      shape = Ecosystem
    ),
    data = hbt_coord,
    # Don't set the size as 0 until you don't want to see anything here.
    # Don't set the size as 0 until you don't want to see anything here.
    # Don't set the size as 0 until you don't want to see anything here.
    size = 0 * scale_parm
  ) +
  # Convert the color as legend class, becasuse the shape legend is legend class.
  # If there is no class conversion, the shape and color will be showed as two legends.
  # Then select colour as guides and site a larger size to make it more readable.
  # Don't know what's these means? Commit below code will show you everything.
  scale_color_discrete(guide = &quot;legend&quot;) +
  guides(colour = guide_legend(override.aes = list(size = .8 * scale_parm)))
</code></pre>
<p>如果对输出结果满意，那么可以跳过下面这一步，直接进行 <code>ggsave</code> 操作保存图像。不过在这里，保存图像之前，我们还需要修改图片的空白区域 margins 来让图像更合适一些。</p>
<pre><code class="language-r">gmap &lt;- gmap +
  theme(plot.margin =
          # Set marigns of figure, the order of parameters is top, right, bottom, left
          unit(
            c(0 * scale_parm, 0 * scale_parm, -.2 * scale_parm, .2 * scale_parm),
            &quot;cm&quot;
          ))
</code></pre>
<p>最后导出图像即可。<code>ggsvae</code> 提供了丰富的参数定义输出的图像。对于需要投稿 SCI 的文章，通常 Author Guidelines 要求提供不低于 300 DPI 的图片文件。如果允许，保存为 PDF 文件会是不错的方法，毕竟通用性和文件大小都能得到很好的满足。</p>
<pre><code class="language-r">ggsave(filename = &quot;gmap.pdf&quot;, plot = gmap, width = 9 * scale_parm, height = 6.2 * 
    scale_parm, units = &quot;cm&quot;, dpi = 600)
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://picgo-1256649726.cos.ap-chengdu.myqcloud.com/gmap.jpg" alt="" loading="lazy"></figure>
<p>最后，如果有任何更好的意见和建议，换用通过任何形式与我交流。祝大家都能制作出令自己（主要是杂志）满意的作品。</p>
<h2 id="参考文献-参考文献">参考文献 [参考文献]</h2>
<div id="refs" class="references hanging-indent" markdown="1">
<div id="ref-bivand_rgdal_2019" markdown="1">
<p>Bivand, Roger, Tim Keitt, Barry Rowlingson, Edzer Pebesma, Michael Sumner, Robert Hijmans, Even Rouault, Frank Warmerdam, Jeroen Ooms, and Colin Rundel. 2019. “Rgdal: Bindings for the ’Geospatial’ Data Abstraction Library.” <a href="https://CRAN.R-project.org/package=rgdal">https://CRAN.R-project.org/package=rgdal</a>.</p>
</div>
<div id="ref-feed_the_future_raster_nodate" markdown="1">
<p>Feed the Future. n.d. “Raster Data — R Spatial.” Blog. <em>Spatial Data Science</em>. Accessed February 19, 2020. <a href="https://rspatial.org/raster/spatial/4-rasterdata.html">https://rspatial.org/raster/spatial/4-rasterdata.html</a>.</p>
</div>
<div id="ref-hijmans_raster_2020" markdown="1">
<p>Hijmans, Robert J., Jacob van Etten, Michael Sumner, Joe Cheng, Andrew Bevan, Roger Bivand, Lorenzo Busetto, et al. 2020. “Raster: Geographic Data Analysis and Modeling.” <a href="https://CRAN.R-project.org/package=raster">https://CRAN.R-project.org/package=raster</a>.</p>
</div>
<div id="ref-hollister_elevatr_2018" markdown="1">
<p>Hollister, Jeffrey, and Tarak Shah. 2018. “Elevatr: Access Elevation Data from Various APIs.” <a href="https://github.com/jhollist/elevatr">https://github.com/jhollist/elevatr</a>.</p>
</div>
<div id="ref-pebesma_sp_2019" markdown="1">
<p>Pebesma, Edzer, Roger Bivand, Barry Rowlingson, Virgilio Gomez-Rubio, Robert Hijmans, Michael Sumner, Don MacQueen, Jim Lemon, Josh O’Brien, and Joseph O’Rourke. 2019. “Sp: Classes and Methods for Spatial Data.” <a href="https://CRAN.R-project.org/package=sp">https://CRAN.R-project.org/package=sp</a>.</p>
</div>
<div id="ref-wickham_ggplot2_2019" markdown="1">
<p>Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and RStudio. 2019. “Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics.” <a href="https://CRAN.R-project.org/package=ggplot2">https://CRAN.R-project.org/package=ggplot2</a>.</p>
</div>
<div id="ref-wickham_tidyverse_2019" markdown="1">
<p>Wickham, Hadley, and RStudio. 2019. “Tidyverse: Easily Install and Load the ’Tidyverse’.” <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>.</p>
</div>
<div id="ref-wickham_scales_2019" markdown="1">
<p>Wickham, Hadley, Dana Seidel, and RStudio. 2019. “Scales: Scale Functions for Visualization.” <a href="https://CRAN.R-project.org/package=scales">https://CRAN.R-project.org/package=scales</a>.</p>
</div>
<div id="ref-__2002" markdown="1">
<p>张镱锂, 李炳元, and 郑度. 2002. “论青藏高原范围与面积.” <em>地理学报</em> 21 (1): 1–8. <a href="https://doi.org/10.11821/yj2002010001">https://doi.org/10.11821/yj2002010001</a>.</p>
</div>
</div>
]]></content>
    </entry>
</feed>